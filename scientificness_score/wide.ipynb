{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edf871d-515c-4c7b-b103-205502d4c824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 19:44:47.387759: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-01 19:44:47.567417: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-01 19:44:48.176680: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-01 19:44:48.176746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-01 19:44:48.176751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9ade0d-799e-477c-a71a-927ebd516bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "from torchmetrics import R2Score\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn import L1Loss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tokenizers\n",
    "\n",
    "import pandas as pd\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41fbe0ff-f1b7-452a-bbb3-16b72b112c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d9f232-fedc-4903-ba71-37be6a16817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA=0.2\n",
    "BATCH_SIZE=4\n",
    "EPOCHS=10\n",
    "LEARNING_RATE = 0.0005\n",
    "DROPOUT = 0.3\n",
    "WEIGHT_DECAY = 0.05\n",
    "tokenizer_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d98554-9e55-48aa-a6d3-d97ce5ca005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCI_PATHS : list() = ['/media/nvme3n1/proj_scisen/datasets/SciSen-as.txt',\n",
    "                        '/media/nvme3n1/proj_scisen/datasets/SciSen-a.txt',\n",
    "                        '/media/nvme3n1/proj_scisen/datasets/SciSen-b.txt',\n",
    "                        '/media/nvme3n1/proj_scisen/datasets/SciSen-c.txt']\n",
    "NON_SCI_PATHS: list() = ['/media/nvme3n1/proj_scisen/datasets/ScifiSen1.txt',\n",
    "                          '/media/nvme3n1/proj_scisen/datasets/RedditSen.txt',\n",
    "                          \"/media/nvme3n1/proj_scisen/datasets/ukraineTweets.txt\",\n",
    "                          '/media/nvme3n1/proj_scisen/datasets/ScifiSen2.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ca70e-f204-4ac7-9080-63e3589cedfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b414367c-fd46-4310-9d31-b541d3b848f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_sentences : list() = []\n",
    "non_sci_sentences : list() = []\n",
    "\n",
    "eval_dict= {0 : None,\n",
    "           1 : None,\n",
    "           2 : None,\n",
    "           3 : None}\n",
    "rank_sentences : list() = []\n",
    "\n",
    "test_size = 500\n",
    "for paths in range(0, len(SCI_PATHS)):\n",
    "    with open(SCI_PATHS[paths]) as f:\n",
    "        for line in f:\n",
    "            rank_sentences.append(line)\n",
    "    sci_sentences = sci_sentences + (rank_sentences[0:int(len(rank_sentences)*0.9)])\n",
    "    eval_dict[paths] = rank_sentences[int(len(rank_sentences)*0.9):len(rank_sentences)]\n",
    "    rank_sentences=[]\n",
    "\n",
    "\n",
    "\n",
    "index = 0\n",
    "for path in NON_SCI_PATHS:\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            non_sci_sentences.append(line) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d527d79d-6df7-4c36-9922-57791919eef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3154189"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sci_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f71f0ae-7f58-400c-8630-b01a001f8273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487443"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_sci_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8770331a-13b9-4281-ad36-dbefd224c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3154189/3154189 [00:02<00:00, 1414541.29it/s]\n"
     ]
    }
   ],
   "source": [
    "input_list : list() = []\n",
    "\n",
    "for i in tqdm(range(0,int(len(sci_sentences)))):\n",
    "    input_list.append({**{'text': sci_sentences[i].rstrip(\"\\n\")}, 'labels':0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3080c85c-66e7-44a9-a58a-391eb61f7eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 487443/487443 [00:00<00:00, 1469624.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,int(len(non_sci_sentences)))):\n",
    "    input_list.append({**{'text': non_sci_sentences[i].rstrip(\"\\n\")}, 'labels':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7719e707-627b-405e-90ef-ba162cca3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.DataFrame(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba4d858-6d9e-4f7c-a86a-533ca6e4cc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "305ec91c-08a7-42eb-a630-23e76fab64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input=df_input.mask(df_input == '')\n",
    "df_input = df_input[~df_input['text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385c49b2-2de3-4c07-bfe4-6e33f883e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (3641632, 2)\n",
      "TRAIN Dataset: (2804057, 2)\n",
      "VALIDATION Dataset: (801159, 2)\n",
      "FULL TEST Dataset: (36416, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.77\n",
    "valid_size = 0.22\n",
    "\n",
    "#split into train, test:\n",
    "trainvalid_df=df_input.sample(frac= train_size + valid_size ,random_state=200)\n",
    "test_dataset=df_input.drop(trainvalid_df.index).reset_index(drop=True)\n",
    "trainvalid_df = trainvalid_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#split into train, validation:\n",
    "train_dataset=trainvalid_df.sample(frac= train_size/(train_size + valid_size) ,random_state=200)\n",
    "valid_dataset=trainvalid_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df_input.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"FULL TEST Dataset: {}\".format(test_dataset.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "537f4cc7-9f00-40dc-a845-45269c60b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(len(train_dataset)) + int(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fa42c29-1cd6-47cf-8729-eb469a3caa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We increase the number of residual blocks to 6...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We propose an end-to-end trained architecture ...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This led to the model generating both that the...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This clearly indicates that a single approach ...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And it is interesting to verify that &lt;equation...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804052</th>\n",
       "      <td>The query result from &lt;equation&gt; from the netw...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804053</th>\n",
       "      <td>Thus, we carefully design FAME as an augmentat...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804054</th>\n",
       "      <td>Do you live on in ignorance (and potentially b...</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804055</th>\n",
       "      <td>To train our contextual knowledge ranker, we s...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804056</th>\n",
       "      <td>However, for the TLS algorithm, the cost is on...</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2804057 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  labels\n",
       "0        We increase the number of residual blocks to 6...     0.9\n",
       "1        We propose an end-to-end trained architecture ...     0.9\n",
       "2        This led to the model generating both that the...     0.9\n",
       "3        This clearly indicates that a single approach ...     0.9\n",
       "4        And it is interesting to verify that <equation...     0.9\n",
       "...                                                    ...     ...\n",
       "2804052  The query result from <equation> from the netw...     0.9\n",
       "2804053  Thus, we carefully design FAME as an augmentat...     0.9\n",
       "2804054  Do you live on in ignorance (and potentially b...     0.1\n",
       "2804055  To train our contextual knowledge ranker, we s...     0.9\n",
       "2804056  However, for the TLS algorithm, the cost is on...     0.9\n",
       "\n",
       "[2804057 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9084bf76-bdf1-48b2-a641-699b92c04b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fb384f2-d907-483d-a450-cdfd4aaf4b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3605216"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78f74727-7eeb-4594-b5c1-9480524511a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding documents without max_length\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(train_labels\u001b[38;5;241m.\u001b[39mdata, valid_labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding documents without max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m enc_docs \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(raw_doc, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m raw_doc \u001b[38;5;129;01min\u001b[39;00m raw_documents]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding labels...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#label2index = {label: idx for idx, label in enumerate(labels_in_order)}\u001b[39;00m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(train_labels\u001b[38;5;241m.\u001b[39mdata, valid_labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding documents without max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m enc_docs \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m raw_doc \u001b[38;5;129;01min\u001b[39;00m raw_documents]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding labels...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#label2index = {label: idx for idx, label in enumerate(labels_in_order)}\u001b[39;00m\n",
      "File \u001b[0;32m/media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2278\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2242\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2243\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2278\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2610\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2600\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2601\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2602\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2603\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2607\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2608\u001b[0m )\n\u001b[0;32m-> 2610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:499\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    478\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    496\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    498\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 499\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:438\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    426\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    427\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    428\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    429\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    440\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    441\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    442\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    443\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    444\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    445\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    446\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    447\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    450\u001b[0m ]\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    458\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:439\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    426\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    427\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    428\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    429\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    438\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_encoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    450\u001b[0m ]\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    458\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/media/nvme3n1/proj_scisen/venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:219\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._convert_encoding\u001b[0;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    217\u001b[0m encoding_dict \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encodings:\n\u001b[0;32m--> 219\u001b[0m     encoding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_token_type_ids:\n\u001b[1;32m    222\u001b[0m         encoding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(e\u001b[38;5;241m.\u001b[39mtype_ids)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = np.array(train_dataset['text'].values.tolist(), dtype=object)\n",
    "valid_data = np.array(valid_dataset['text'].values.tolist(), dtype=object)\n",
    "raw_documents = np.append(train_data, valid_data)\n",
    "\n",
    "train_labels = np.array(train_dataset['labels'].values.tolist(), dtype=object)\n",
    "test_labels_dict = dict()\n",
    "valid_labels = np.array(valid_dataset['labels'].values.tolist(), dtype=object)\n",
    "labels = np.append(train_labels.data, valid_labels.data)\n",
    "\n",
    "\n",
    "print(f\"Encoding documents without max_length\")\n",
    "enc_docs = [tokenizer.encode(raw_doc, truncation=True, max_length=512) for raw_doc in raw_documents]\n",
    "\n",
    "print(\"Encoding labels...\")\n",
    "#label2index = {label: idx for idx, label in enumerate(labels_in_order)}\n",
    "\n",
    "enc_labels = []\n",
    "idx = 0\n",
    "labels_in_order = [\"score\"]\n",
    "label2index = {label: idx for idx, label in enumerate(labels_in_order)}\n",
    "\n",
    "train_mask, valid_mask = torch.zeros(N, dtype=torch.bool), torch.zeros(N, dtype=torch.bool)\n",
    "\n",
    "for array in train_labels:\n",
    "    label_names = np.array(array)\n",
    "    array_ids = np.empty(1)\n",
    "    array_ids.fill(0)\n",
    "    train_mask[idx] = True\n",
    "    idx += 1\n",
    "    #for label_name in label_names:\n",
    "    #  for label, index in label2index.items():\n",
    "    #        if label_name == label:\n",
    "    #            array_ids[index] = 1\n",
    "    enc_labels.append(array)\n",
    "        \n",
    "for array in valid_labels:\n",
    "    label_names = np.array(array)\n",
    "    array_ids = np.empty(1)\n",
    "    array_ids.fill(0)\n",
    "    valid_mask[idx] = True\n",
    "    idx += 1\n",
    "    #for label_name in label_names:\n",
    "    #    for label, index in label2index.items():\n",
    "    #        if label_name == label:\n",
    "    #            array_ids[index] = 1\n",
    "    enc_labels.append(array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109c24c-e938-466b-b6a4-9094bca4d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_docs_arr, enc_labels_arr = np.array(enc_docs, dtype='object'), np.array(enc_labels)\n",
    "\n",
    "train_data = list(zip(enc_docs_arr[train_mask], enc_labels_arr[train_mask]))\n",
    "valid_data = list(zip(enc_docs_arr[valid_mask], enc_labels_arr[valid_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92c462c7-38e2-4af6-ba3f-f14e33fed2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_for_mlp(list_of_samples):\n",
    "    \"\"\" Collate function that creates batches of flat docs tensor and offsets \"\"\"\n",
    "    offset = 0\n",
    "    flat_docs, offsets, labels = [], [], []\n",
    "    for doc, label in list_of_samples:\n",
    "        if isinstance(doc, tokenizers.Encoding):\n",
    "            doc = doc.ids\n",
    "        offsets.append(offset)\n",
    "        flat_docs.extend(doc)\n",
    "        labels.append(label)\n",
    "        offset += len(doc)\n",
    "\n",
    "    return torch.tensor(np.array(flat_docs)), torch.tensor(np.array(offsets)), torch.tensor(np.array(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdcc34ab-0964-401e-8518-13481afe9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10aa380-cbec-4699-ab04-c544e25597ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_classes,\n",
    "                 num_hidden_layers=1,\n",
    "                 hidden_size=1024, hidden_act='relu',\n",
    "                 dropout=0.5, idf=None, mode='mean',\n",
    "                 pretrained_embedding=None, freeze=True,\n",
    "                 embedding_dropout=0.5):\n",
    "        nn.Module.__init__(self)\n",
    "        # Treat TF-IDF mode appropriately\n",
    "        mode = 'sum' if idf is not None else mode\n",
    "        self.idf = idf\n",
    "\n",
    "        # Input-to-hidden (efficient via embedding bag)\n",
    "        if pretrained_embedding is not None:\n",
    "            # vocabsize is defined by embedding in this case\n",
    "            self.embed = nn.EmbeddingBag.from_pretrained(pretrained_embedding, freeze=freeze, mode=mode)\n",
    "            embedding_size = pretrained_embedding.size(1)\n",
    "            self.embedding_is_pretrained = True\n",
    "        else:\n",
    "            assert vocab_size is not None\n",
    "            self.embed = nn.EmbeddingBag(vocab_size, hidden_size, mode=mode)\n",
    "            embedding_size = hidden_size\n",
    "            self.embedding_is_pretrained = False\n",
    "\n",
    "        self.activation = getattr(F, hidden_act)\n",
    "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden-to-hidden\n",
    "        for i in range(num_hidden_layers - 1):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(embedding_size, hidden_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        # Hidden-to-output\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "        #self.layers.append(nn.Softmax(hidden_size if self.layers else embedding_size, num_classes))\n",
    "        #self.layers.append(nn.Softmax())\n",
    "        # Loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def forward(self, input, offsets, labels=None):\n",
    "        # Use idf weights if present\n",
    "        \n",
    "        \n",
    "        idf_weights = self.idf[input] if self.idf is not None else None\n",
    "\n",
    "        h = self.embed(input, offsets, per_sample_weights=idf_weights)\n",
    "        if self.idf is not None:\n",
    "            # In the TF-IDF case: renormalize according to l2 norm\n",
    "            h = h / torch.linalg.norm(h, dim=1, keepdim=True)\n",
    "\n",
    "        if not self.embedding_is_pretrained:\n",
    "            # No nonlinearity when embedding is pretrained\n",
    "            h = self.activation(h)\n",
    "\n",
    "        h = self.embedding_dropout(h)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # at least one\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                # No activation/dropout for final layer\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "\n",
    "        if labels is not None: \n",
    "            labels = labels.to(torch.float64)\n",
    "            h = h.to(torch.float64)\n",
    "            loss = self.loss_function(h.squeeze(), labels)            \n",
    "            return loss, h\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b9dce-5b07-4d64-983a-a5dabf1303b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52b72124-c4fe-41c4-990e-73cfb199f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjustinmuecke\u001b[0m (\u001b[33mscise\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nvme3n1/proj_scisen/git_repos/git_repos_justin/22_scisen/scientificness_score/wandb/run-20230301_194807-20rhejml</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/scise/wideclassifier/runs/20rhejml\" target=\"_blank\">WideMLP-FULL</a></strong> to <a href=\"https://wandb.ai/scise/wideclassifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)  \n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "import wandb\n",
    "WANDB = True  \n",
    "model_id = \"WideMLP_2s_32_0.1\" #for filenames\n",
    "wandb_id = \"WideMLP-FULL\"\n",
    "wandb_project = \"wideclassifier\"\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(project=wandb_project, name = wandb_id, config={\"epochs\": EPOCHS, \"dropout\": DROPOUT, \"batch_size\": BATCH_SIZE, \"learning_rate\": LEARNING_RATE, \"lambdas\": LAMBDA, \"trainingdata\":\"full\", })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ee84802-ec3f-47a9-bed2-b8478561eb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ddf841b-06af-46d7-9bab-bcff913daf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MLP\n"
     ]
    }
   ],
   "source": [
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LOGGING_STEPS = 50  \n",
    "torch_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Initializing MLP\")\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = MLP(vocab_size, 1)\n",
    "\n",
    "model.to(torch_device)\n",
    "if WANDB:\n",
    "    wandb.watch(model, log_freq=LOGGING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2b77ff4-8c8c-4ab2-a6ae-77f2092c3dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_collate_for_transformer(pad_token_id):\n",
    "    \"\"\" Closure to include padding in collate function \"\"\"\n",
    "    def _collate_for_transformer(examples):\n",
    "        docs, labels = list(zip(*examples))\n",
    "        input_ids = torch.tensor(pad(docs, with_token=pad_token_id))\n",
    "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "        attention_mask[input_ids == pad_token_id] = 0\n",
    "        labels = torch.tensor(labels)\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "        return input_ids, attention_mask, token_type_ids, labels\n",
    "    return _collate_for_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d026903-79a1-4537-9220-fa2d07d47e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebbab3-7772-412e-9df6-d98e97d1c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate_fn = get_collate_for_transformer(tokenizer.pad_token_id)\n",
    "collate_fn = collate_for_mlp\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=('cuda' in torch_device))\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=('cuda' in torch_device))\n",
    "\n",
    "# len(train_loader) no of batches\n",
    "t_total = len(train_loader) // GRADIENT_ACCUMULATION_STEPS * EPOCHS\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                                num_training_steps=t_total)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Train!\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_data))\n",
    "logger.info(\"  Num Epochs = %d\", EPOCHS)\n",
    "logger.info(\"  Batch size  = %d\", BATCH_SIZE)\n",
    "logger.info(\"  Total train batch size (w. accumulation) = %d\",\n",
    "            BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "logger.info(\"  Gradient Accumulation steps = %d\", GRADIENT_ACCUMULATION_STEPS)\n",
    "logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "global_step = 0\n",
    "loss_vals = []\n",
    "tr_loss, logging_loss, nb_val_steps, vl_loss = 0.0, 0.0, 0.0, 0.0\n",
    "model.zero_grad()\n",
    "#train_iterator = trange(EPOCHS, desc=\"Epoch\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=\"Iteration\")):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(torch_device) for t in batch)\n",
    "        \n",
    "        outputs = model(batch[0], batch[1], batch[2])\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "            if WANDB:\n",
    "                wandb.log({'epoch': epoch,\n",
    "                            'lr': scheduler.get_last_lr()[0],\n",
    "                            'loss': loss})\n",
    "\n",
    "        if LOGGING_STEPS > 0 and global_step % LOGGING_STEPS == 0:\n",
    "            avg_loss = (tr_loss - logging_loss) / LOGGING_STEPS\n",
    "            writer.add_scalar('lr', scheduler.get_last_lr()[0], global_step)\n",
    "            writer.add_scalar('loss', avg_loss, global_step)\n",
    "            logging_loss = tr_loss\n",
    "\n",
    "    for step, batch in enumerate(tqdm(valid_loader, desc=\"Validating\")):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(torch_device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[0], batch[1], batch[2])\n",
    "\n",
    "        nb_val_steps += 1\n",
    "        loss, logits = outputs[:2]\n",
    "        vl_loss += loss.mean().item()\n",
    "\n",
    "    vl_loss /= nb_val_steps\n",
    "    loss_vals.append(vl_loss)\n",
    "    if WANDB:\n",
    "        wandb.log({'val_loss': vl_loss})\n",
    "\n",
    "writer.close()\n",
    "#return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae3e3a-0cad-4ddf-a143-de678137e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"Models/WideMLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26bf47fd-c034-4486-806d-b54dced48b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec49db2d-d81f-4afe-987d-47869fa1d237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 1.6026e-04,  7.2114e-04,  1.2834e-03,  ...,  1.1757e-03,\n",
       "                        1.0334e-03,  1.1002e-08],\n",
       "                      [-1.9059e-39, -9.9950e-39,  6.0637e-39,  ...,  1.7448e-38,\n",
       "                        6.2921e-39, -1.4666e-38],\n",
       "                      [-9.7377e-39,  6.1381e-39,  7.5940e-39,  ..., -2.3575e-39,\n",
       "                       -7.7914e-39,  2.4283e-39],\n",
       "                      ...,\n",
       "                      [ 9.5778e-39,  5.5740e-39,  1.0309e-39,  ..., -1.0942e-39,\n",
       "                       -1.0446e-39,  1.5163e-38],\n",
       "                      [-1.6233e-38,  7.6666e-39, -8.2495e-40,  ..., -2.3722e-39,\n",
       "                        1.0887e-38,  6.5681e-39],\n",
       "                      [ 6.2092e-39, -4.3323e-40,  1.2024e-38,  ..., -1.0052e-38,\n",
       "                        1.0064e-38, -1.2710e-38]], device='cuda:0')),\n",
       "             ('layers.0.weight',\n",
       "              tensor([[ 0.1001,  0.0735,  0.0661,  ...,  0.0705,  0.1022, -0.4329]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.bias', tensor([0.3263], device='cuda:0'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2245d39b-0fd9-4e63-8561-0b4722b59451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalRanks(input):\n",
    "    \n",
    "    enc_in = [tokenizer.encode(sentence, truncation=True, max_length=512) for sentence in input]\n",
    "    test_labels = np.array([0.9 for i in range(len(input))] , dtype=object)\n",
    "    enc_labels = []\n",
    "    \n",
    "    idx = 0\n",
    "    N=len(input)\n",
    "    train_mask, valid_mask = torch.zeros(N, dtype=torch.bool), torch.zeros(N, dtype=torch.bool)\n",
    "\n",
    "    for array in test_labels:\n",
    "        label_names = np.array(array)\n",
    "        array_ids = np.empty(1)\n",
    "        array_ids.fill(0)\n",
    "        train_mask[idx] = True\n",
    "        idx += 1\n",
    "        enc_labels.append(array)\n",
    "        \n",
    "        \n",
    "    enc_docs_arr, enc_labels_arr = np.array(enc_in, dtype='object'), np.array(enc_labels)\n",
    "    test_data = list(zip(enc_docs_arr[train_mask], enc_labels_arr[train_mask]))\n",
    "    \n",
    "    return test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46b5849c-eecb-4872-80d9-fe43da6a0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def predictValues(test_data : List) -> List[int]:\n",
    "    data = collate_for_mlp(test_data)\n",
    "    tensor = model(data[0].to(torch_device), data[1].to(torch_device))\n",
    "    return [tensor.detach().cpu().numpy()[entry][0] for entry in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c857a12-67b8-4db8-9dec-ddc229af6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(values):\n",
    "    averageScore = 0.0\n",
    "    for score in values:\n",
    "        averageScore = averageScore + score\n",
    "    averageScore = averageScore / len(values)\n",
    "    return averageScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8847ed3a-6c5c-41df-8a34-359954b34ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def standDev(values, averageScore):\n",
    "    sd = 0.0\n",
    "    for score in values:\n",
    "        sd = sd + abs(averageScore - score)**2 \n",
    "    sd = sd/len(values)\n",
    "    sd = math.sqrt(sd)\n",
    "    return sd\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dceffd6e-05c8-48aa-ab74-9715dcc2c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(values, avg, sd):\n",
    "    with open(\"SentenceEvals/WideMLP-4\", \"w\") as target: \n",
    "        target.write(\"Mean :\\n\" + str(avg) +\"\\n\")\n",
    "        target.write(\"Standard Deviation:\\n\" + str(sd) + \"\\n\\n\")\n",
    "        target.write(\"Values:\\n\" + str(values) + \"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d103b-33fd-43d8-9c9e-2df437bdb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(values, rank):\n",
    "    \n",
    "    vals = [0.0 for i in range(0,1000)]\n",
    "    entries : int = 0\n",
    "\n",
    "    for score in values:\n",
    "        score = float(score) * 1000 \n",
    "        score = int(score)\n",
    "        if score > 999:\n",
    "            score = 999\n",
    "        \n",
    "        vals[score] = vals[score] + 1\n",
    "        entries = entries + 1\n",
    "\n",
    "    x_axis = [i for i in range(0,1000)]\n",
    "    y_axis = vals\n",
    "    plt.bar(x_axis, y_axis)\n",
    "    plt.title('Value Distributions')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Occurance')\n",
    "    plt.savefig(\"Plots/WideMLP\" + \"-\" + str(rank) + \".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a4a8d18-fdbb-47aa-943b-6cc3ca51ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Analysis(object):\n",
    "    def __init__(self, values, average, sd):\n",
    "        self.values = values\n",
    "        self.average = average\n",
    "        self.sd = sd\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return __reps__(self)\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.stream.close()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"avg: \" + str(self.average) + \"\\nStandDev: \" + str(self.sd) + \"\\nValues: \" + str(self.values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bb845-eb55-4455-b793-011be799ab9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0444df3-297d-4226-931f-0c374ff109bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank in range(len(eval_dict)):\n",
    "    data = evalRanks(eval_dict[rank])\n",
    "    values = predictValues(data)\n",
    "    values = sorted(values)\n",
    "    averageScore = average(values)\n",
    "    deviation = standDev(values, averageScore)\n",
    "    result = Analysis(values, averageScore, deviation) \n",
    "    file = open(\"Pickled_Object/WideMLP-4\", \"wb\")\n",
    "    pickle.dump(result, file)\n",
    "    export(values, averageScore, deviation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab801fad-2af1-4129-ba25-daa56fd38ad0",
   "metadata": {},
   "source": [
    "# Investigate How Tokens influence Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37f1bd1f-276f-42f5-be5e-c64248d2a18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NON_SCI_PATH = '/media/nvme3n1/proj_scisen/datasets/ScifiSen9.txt'\n",
    "model.load_state_dict(torch.load(\"Models/WideMLP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d07f67a-11b1-4c66-989c-99510f259293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def insert_token(sentence): \n",
    "    x = random.randint(0,99)\n",
    "    if x % 2 == 0:\n",
    "        sentence_arr = sentence.split(\" \") \n",
    "        sentence_arr.insert(random.randint(0, len(sentence_arr)), \"<equation>\")\n",
    "        return \" \".join(sentence_arr)\n",
    "    else:\n",
    "        sentence_arr = sentence.split(\" \") \n",
    "        sentence_arr.insert(random.randint(0, len(sentence_arr)), \"<citation>\")\n",
    "        return \" \".join(sentence_arr)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b06e32e5-af01-49ac-8f33-36bcfd3172e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_sentences: list() = []\n",
    "original_sentences : list() = []\n",
    "\n",
    "eval_dict= {0 : None}\n",
    "           #,1 : None}\n",
    "\n",
    "with open(NON_SCI_PATH) as f:\n",
    "    for line in f:\n",
    "        original_sentences.append(line) \n",
    "        modified_sentences.append(insert_token(line))\n",
    "eval_dict[0] = original_sentences\n",
    "eval_dict[1] = modified_sentences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0f65554-821f-487a-a87b-2286f9b3296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank in range(len(eval_dict)):\n",
    "    data = evalRanks(eval_dict[rank])\n",
    "    values = predictValues(data)\n",
    "    values = sorted(values)\n",
    "    averageScore = average(values)\n",
    "    deviation = standDev(values, averageScore)\n",
    "    result = Analysis(values, averageScore, deviation) \n",
    "    file = open(\"Pickled_Object/WideMLP-4\", \"wb\")\n",
    "    pickle.dump(result, file)\n",
    "    export(values, averageScore, deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fddbfe-3e48-4910-b24b-28b583932c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8344acd-af9e-4487-bebe-a51e1288cef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
