{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888eaf7f-8ed6-4c39-89d5-6adead3c5648",
   "metadata": {},
   "outputs": [],
   "source": [
    " # dependencies\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import transformers\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "import re\n",
    "import wandb\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ce1fef-e187-4306-ae6b-e58699b56497",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a1b01c-e95b-460a-b34a-b598d3ee6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b6f873-7cbf-462d-ac43-21b5ba6fc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"Models/SciBertFull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879a5ded-d31d-431a-9ae5-afbf99f99f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "# to create training and validation dataset\n",
    "# input: (BERT) tokenizer, dataframe, max_length\n",
    "# output: tokenized outputs (ids, attention_mask, token_type_ids) and tags used for BERT training\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = dataframe.label\n",
    "        #self.targets = self.data.llist\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.text[index],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            #pad_to_max_length=True,\n",
    "            padding = \"max_length\",\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22af55db-0f88-408c-8b83-eaec1c8f0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_SCI_PATH = \"/media/nvme3n1/proj_scisen/datasets/ScifiSen9.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520e7e11-65f0-47e7-a40b-7a24a1895b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def insert_token(sentence): \n",
    "    x = random.randint(0,99)\n",
    "    if x % 2 == 0:\n",
    "        sentence_arr = sentence.split(\" \") \n",
    "        sentence_arr.insert(random.randint(0, len(sentence_arr)), \"<equation>\")\n",
    "        return \" \".join(sentence_arr)\n",
    "    else:\n",
    "        sentence_arr = sentence.split(\" \") \n",
    "        sentence_arr.insert(random.randint(0, len(sentence_arr)), \"<reference>\")\n",
    "        return \" \".join(sentence_arr)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ea4b17-4c2b-4c34-adcc-67687d597b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict= dict()\n",
    "sentences = []\n",
    "modified_sentences = []\n",
    "with open(NON_SCI_PATH) as f:\n",
    "    for line in f:\n",
    "        sentences.append(line)\n",
    "        modified_sentences.append(insert_token(line))\n",
    "    eval_dict[0] = sentences\n",
    "    eval_dict[1] = modified_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08153cfb-1306-4d37-931d-3d6e6d59688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(sci_sentences):\n",
    "    input_list : list() = []\n",
    "\n",
    "    for i in tqdm(range(0,int(len(sci_sentences)))):\n",
    "        input_list.append({**{'text': sci_sentences[i].rstrip(\"\\n\")}, 'label':0.1})\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e59cb86-515d-489d-abcc-68d93d08401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100001/100001 [00:00<00:00, 1176570.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100001/100001 [00:00<00:00, 1262596.61it/s]\n"
     ]
    }
   ],
   "source": [
    "labeled = dict()\n",
    "for rank in eval_dict:\n",
    "    labeled[rank] = label(eval_dict[rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508378a0-7a3b-4803-bcff-e0e64d590756",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = dict()\n",
    "for rank in labeled: \n",
    "    eval_df[rank] = pd.DataFrame(labeled[rank])\n",
    "    \n",
    "testing_sets = dict()\n",
    "for rank in eval_df:\n",
    "    testing_sets[rank] = CustomDataset(eval_df[rank], tokenizer, 512)\n",
    "    \n",
    "test_params = {'batch_size': 4,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "\n",
    "testing_loaders = dict()\n",
    "for rank in eval_df:\n",
    "    testing_loaders[rank] = DataLoader(testing_sets[rank], **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb557706-64a9-4847-b6f9-75cec220ce45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 0 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25001/25001 [05:34<00:00, 74.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25001/25001 [05:37<00:00, 74.12it/s]\n"
     ]
    }
   ],
   "source": [
    "model.to(torch_device)\n",
    "\n",
    "eval_loaders = testing_loaders\n",
    "outputs_by_rank = dict()\n",
    "targets_by_rank = dict()\n",
    "\n",
    "for evalset in eval_loaders:\n",
    "    print(f\"--------------- {evalset} ---------------\")\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(tqdm(eval_loaders[evalset], 0)):\n",
    "            ids = data['ids'].to(torch_device, dtype=torch.long)\n",
    "            mask = data['mask'].to(torch_device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(torch_device, dtype = torch.long)\n",
    "            targets = data['targets'].to(torch_device, dtype=torch.float)\n",
    "            outputs = model(ids, mask,token_type_ids)\n",
    "            fin_outputs = fin_outputs + [out for out in outputs[0].detach().cpu().numpy()]\n",
    "    outputs_by_rank[evalset] = fin_outputs\n",
    "            #fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            #fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "    #outputs = fin_outputs\n",
    "    #targets = fin_targets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ea0052-2636-457a-98ca-3c684d50f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Analysis(object):\n",
    "    def __init__(self, values, average, sd):\n",
    "        self.values = values\n",
    "        self.average = average\n",
    "        self.sd = sd\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return __reps__(self)\n",
    "\n",
    "    def __exit__(self, type, value, tb):\n",
    "        self.stream.close()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"avg: \" + str(self.average) + \"\\nStandDev: \" + str(self.sd) + \"\\nValues: \" + str(self.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2b4a2c-4f6b-4396-a7ad-91c50e565661",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "\n",
    "for rank in outputs_by_rank:\n",
    "    score_list : [int] = list()\n",
    "    for score in outputs_by_rank[rank]:\n",
    "        score_list = score_list + [score[0]]\n",
    "    scores[rank] = score_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a44f0b5-e738-4648-8a53-4879ed07aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(values):\n",
    "    averageScore = 0.0\n",
    "    for score in values:\n",
    "        averageScore = averageScore + score\n",
    "    averageScore = averageScore / len(values)\n",
    "    return averageScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28f32bcd-f177-4ad8-93f8-0a0ec1b7f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def standDev(values, averageScore):\n",
    "    sd = 0.0\n",
    "    for score in values:\n",
    "        sd = sd + abs(averageScore - score)**2 \n",
    "    sd = sd/len(values)\n",
    "    sd = math.sqrt(sd)\n",
    "    return sd\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c07d81c9-a5d4-40f3-814b-50c900ccc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export(values, avg, sd):\n",
    "    with open(\"SentenceEvals/SciBert-token-\"+str(rank), \"w\") as target: \n",
    "        target.write(\"Mean :\\n\" + str(avg) +\"\\n\")\n",
    "        target.write(\"Standard Deviation:\\n\" + str(sd) + \"\\n\\n\")\n",
    "        target.write(\"Values:\\n\" + str(values) + \"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eacc4c06-7e57-4c76-9aa7-32cde0071cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank in scores:\n",
    "    values = sorted(scores[rank])\n",
    "    averageScore = average(values)\n",
    "    deviation = standDev(values, averageScore)\n",
    "    result = Analysis(values, averageScore, deviation) \n",
    "    file = open(\"Pickled_Object/SciBert-token\" + str(rank), \"wb\")\n",
    "    pickle.dump(result, file)\n",
    "    export(values, averageScore, deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd8134-b77f-4d50-adef-63d2521f0d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
