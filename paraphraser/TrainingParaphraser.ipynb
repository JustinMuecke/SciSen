{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5ccfba-7709-4de7-ae23-133bcb6891ad",
   "metadata": {},
   "source": [
    "# Config\n",
    "the model to train and the dataset to train on can be selected by changing the index in the given list. \n",
    "We provide a approximate batch_size for an Nvidia A-100 with 40GB memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95a994-ddc8-43db-8b1d-877986bf02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use only in notebooks if converted to python file this needs to get removed\n",
    "%env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3c197-a54f-4e0e-94f1-73120ee20703",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda:0'\n",
    "\n",
    "model_checkpoint, batch_size = [[(\"google/t5-v1_1-small\",200),(\"google/t5-v1_1-base\",140),(\"google/t5-v1_1-large\",16)],\n",
    "                [(\"facebook/bart-base\",200),(\"facebook/bart-large\",40)],\n",
    "                [(\"gpt2-medium\",100),(\"gpt2-large\",20)]] [0][0]\n",
    "\n",
    "#batch_size = 70 #force different batchsize if GPU not empty\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "print('model: ',model_name)\n",
    "dataset_name= ['para-1-1-small','para-1-1','idm-small','idm'][3]\n",
    "print('dataset: ',dataset_name)\n",
    "\n",
    "path = '/media/data3/proj_scisen/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ace11a-ed94-4ae1-ad25-e8c8501e0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "max_input_length = 100\n",
    "max_target_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc47e6-6f80-42fa-bdc0-f22019816e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate=2e-5 #default 2e-5\n",
    "weight_decay=0.001 #default 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e391cd0-bb5e-4fc5-9524-883e0538bbd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train model with modified input\n",
    "https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb\n",
    "\n",
    "https://huggingface.co/gpt2-large#how-to-get-started-with-the-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c31847-8b05-416f-915c-54b7e90e6090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer , GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "if 'gpt' in model_name:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
    "    model = GPT2Model.from_pretrained(model_checkpoint)\n",
    "\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "model.to(torch_device)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf37a09-1b1e-4486-8217-4fffb6f3d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if 'gpt' in model_name:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model_inputs = tokenizer(examples[\"para-1-1\"],padding=\"max_length\", max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"text\"],padding=\"max_length\", max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2584f88-380e-4471-bc64-b77b61de5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "try:\n",
    "    tokenized_datasets = load_from_disk(f'{path}datasets/style/tokenized/{dataset_name}-{model_name}')\n",
    "    print('load already tokenized dataset')\n",
    "except FileNotFoundError:\n",
    "    print('load and tokenize dataset')\n",
    "    dataset = load_from_disk(f'{path}datasets/style/{dataset_name}')\n",
    "    if('idm' in dataset_name): #TODO change code to avoid this workaround \n",
    "        dataset = dataset.rename_column('idm','para-1-1')\n",
    "        dataset = dataset.shuffle(seed=42)\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_datasets.save_to_disk(f'{path}datasets/style/tokenized/{dataset_name}-{model_name}')\n",
    "    \n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de505fc6-b635-4e05-b350-77733d637a7e",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a531255-1eb2-4dd7-af12-cf0f44129a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{path}models/style/{model_name}-finetuned-{dataset_name}-lr-{learning_rate}-wd-{weight_decay}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=weight_decay,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=False,\n",
    "    eval_accumulation_steps=1,\n",
    "    remove_unused_columns=True,\n",
    "    auto_find_batch_size =False,\n",
    ")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cfd4d4-a518-4024-905c-1855ef9f667d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "#https://huggingface.co/metrics\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    metric_bleu = load_metric(\"bleu\") \n",
    "    metric_self_bleu = load_metric(\"bleu\") \n",
    "    metric_rouge = load_metric(\"rouge\") \n",
    "    metric_meteor = load_metric(\"meteor\") \n",
    "    metric_bertscore = load_metric(\"bertscore\") \n",
    "    metric_ppl = load_metric(\"perplexity\") \n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for entry in range(len(decoded_preds)):\n",
    "        x_out = decoded_preds[entry].lower().split(' ')\n",
    "        x_ref = [x.lower().split(' ') for x in [decoded_labels[entry]]]\n",
    "\n",
    "        metric_bleu.add_batch(predictions = [x_out], references= [x_ref])\n",
    "        metric_meteor.add_batch(predictions = [x_out], references= [x_ref])\n",
    "        metric_bertscore.add_batch(predictions = [x_out], references= [x_ref],)\n",
    "    result['bleu'] =  metric_bleu.compute()['bleu']\n",
    "    result['meteor']= metric_meteor.compute()['meteor']\n",
    "    result['bertscore']= np.mean(metric_bertscore.compute(model_type='allenai/scibert_scivocab_uncased')['f1'])\n",
    "    result['perplexity']= metric_ppl.compute(input_texts = [x.lower().split(' ') for x in decoded_preds if (len(x.lower().split(' '))>2) ], model_id='allenai/scibert_scivocab_uncased',add_start_token=False)['mean_perplexity']\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cc417-f16e-4432-98e8-d33a37fa7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37808aa-5130-4f37-820f-b6687dd94c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,    \n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17235834-a60e-472b-b4aa-22e7b4329e79",
   "metadata": {},
   "source": [
    "### Start training process\n",
    "The models are saved under \"../proj_scisen/models/style/{model_name}-finetuned-{dataset_name}-lr-{learning_rate}-wd-{weight_decay}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea9ecc-a28e-4481-a739-1f4e3b50a7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
