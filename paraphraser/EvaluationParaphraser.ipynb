{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e791f496-bbf5-4e8e-9f10-86b062d78bb8",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7ca5e-c13f-4640-98f4-ee8196409626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_checkpoint, batch_size = [\n",
    "    [(\"google/t5-v1_1-small\",200),(\"google/t5-v1_1-base\",180),(\"google/t5-v1_1-large\",70)],\n",
    "    [(\"facebook/bart-base\",300),(\"facebook/bart-large\",40)],\n",
    "    [(\"gpt2-medium\",100),(\"gpt2-large\",20)],\n",
    "    [(\"tuner007/pegasus_paraphrase\",100)],\n",
    "    [(\"idm\",100)],\n",
    "    [(\"identity\",100)]] [1][0]\n",
    "\n",
    "#batch_size = math.ceil(batch_size/3) #force different batchsize if GPU not empty\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "print('model: ',model_name)\n",
    "dataset_name= ['para-1-1-small','para-1-1','idm-small','idm'][3]\n",
    "print('dataset: ',dataset_name)\n",
    "torch_device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "learning_rate = 2e-05 #'-' for GPT or identity\n",
    "weight_decay= 0.001   #'-' for GPT\n",
    "\n",
    "model_output_name =f'{model_name}-finetuned-{dataset_name}-lr-{learning_rate}-wd-{weight_decay}'\n",
    "model_path = f\"/media/data3/proj_scisen/models/style/{model_output_name}\"\n",
    "dataset_path = '/media/nvme3n1/proj_scisen/datasets/'\n",
    "output_path = '/media/data3/proj_scisen/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a707a0e-46c5-43e9-8bc9-e3f3f828078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove if converted to python file\n",
    "if 't5' in model_name or 'bart' in model_name:\n",
    "    %env PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49051458-f2f5-4935-8faf-68df478b7b6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load (fine-tuned) model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee2b57-55c5-494d-8737-1ba8a8d7a4b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pegasus Paraphraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a44647-6946-44b9-9518-cbd93bfa48eb",
   "metadata": {},
   "source": [
    "load model and create method for sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7f79c-8fe4-4d0d-88fb-2705cf73fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pegasus' in model_name:\n",
    "    import torch\n",
    "    from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    model.to(torch_device)\n",
    "\n",
    "#create sample outputs for single sentences\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebee106-2095-4c5d-a20a-592da6311589",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045e4d3-bc95-4e78-8665-8cb317cbbf93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'idm' in model_name:\n",
    "    import random\n",
    "    from transformers import pipeline\n",
    "    from transformers.pipelines.pt_utils import KeyDataset\n",
    "    import datasets\n",
    "    from datasets import Dataset\n",
    "\n",
    "    unmasker = pipeline('fill-mask', model='bert-base-uncased',device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280d817-009d-4cf8-9032-ed01d6162c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_replace(sentences,replace=1,add_mask=True):#example: sentences=['Sentence one.','I am a sentence.']\n",
    "    #create batch of sentence to allow batched unmasking if required \n",
    "    new_sentences=[]\n",
    "    for sentence in sentences:\n",
    "        #unmask only accepts one MASK token, this is a problem if the original sentence already contains one \n",
    "        #handle [MASK] like any other word\n",
    "        sentence = sentence.replace('[MASK]','MASK')\n",
    "            \n",
    "        split = sentence.split(' ')\n",
    "        \n",
    "        selected_word = random.randrange(0, len(split))\n",
    "\n",
    "        #find words larger than three characters as long as such words exits and only if we want to replace/delete a word\n",
    "        while (len(split[selected_word]) < 4) and (replace > 0) and (max([len(x) for x in split])>3):\n",
    "            selected_word = random.randrange(0, len(split))\n",
    "            \n",
    "        split1 = split[:selected_word]\n",
    "        split2 = split[selected_word+replace:]\n",
    "\n",
    "        if add_mask:\n",
    "            sentence = ' '.join(split1 +['[MASK]']+ split2) # unmasker fehlt\n",
    "        else:\n",
    "            sentence = ' '.join(split1 + split2)\n",
    "\n",
    "        new_sentences.append(sentence)\n",
    "          \n",
    "    if add_mask:\n",
    "        #join here -> later only unmask or return\n",
    "        my_dict = {\"text\": new_sentences}\n",
    "        dataset_sentences = Dataset.from_dict(my_dict)\n",
    "\n",
    "        out_sentences=[]\n",
    "        unmasked_sentences = unmasker(KeyDataset(dataset_sentences, \"text\"), batch_size=1000,top_k=3,)\n",
    "        for idx, sentence in enumerate(unmasked_sentences):\n",
    "            #ensure that the sentence changed\n",
    "            if (sentences[idx]!=sentence[0]['sequence']) and ('\\'' not in sentence[0]['sequence']):\n",
    "                out_sentences.extend([sentence[0]['sequence']])\n",
    "            elif (sentences[idx]!=sentence[1]['sequence']) and ('\\'' not in sentence[1]['sequence']):\n",
    "                out_sentences.extend([sentence[1]['sequence']])\n",
    "            else:\n",
    "                out_sentences.extend([sentence[2]['sequence']])\n",
    "        return out_sentences\n",
    "    else:\n",
    "        return [out for out in new_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42effe4-0b79-4ccd-8e2c-b50371df2e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modify_delete(sentence):\n",
    "    return modify_replace(sentence,add_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69f5dd-b78f-44b6-b889-3edba09a69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_insert(sentence):\n",
    "    return modify_replace(sentence,replace = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca4810-154e-4b6a-b48d-c4d13da1a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify(sentences):# sentences=['Sentence one.','I am a sentence.']\n",
    "    sentences = np.array(sentences)\n",
    "\n",
    "    max_changes_all = math.ceil(len(sentences[0].split(' '))/2)\n",
    "    buckets = [(random.randrange(0,6)/10 ,len(x.split(' '))) for x in sentences]\n",
    "    changes_bucket = [math.floor(length/2)*changes for changes, length in buckets]    \n",
    "        \n",
    "    for changes in range(0,max_changes_all):\n",
    "        #change all senteces that have not the required amount of changes for their bucket\n",
    "        selected_sentences = [bucket >= changes for bucket in changes_bucket]\n",
    "        r = random.randrange(0,3)\n",
    "        if r==0:\n",
    "            sentences[selected_sentences] =  modify_replace(sentences[selected_sentences])\n",
    "        elif r==1:\n",
    "            sentences[selected_sentences] =  modify_delete(sentences[selected_sentences])\n",
    "        else:\n",
    "            sentences[selected_sentences] =  modify_insert(sentences[selected_sentences])\n",
    "    return sentences, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cea4c-6fe8-4396-bc51-40c93a81c896",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T5 or BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceacfa06-f68f-45ad-af37-9e44138c29c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 't5' in model_name or 'bart' in model_name:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "    import torch\n",
    "    import os\n",
    "    import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e538757-f0a3-4a28-b4ef-22c07e1b7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 't5' in model_name or 'bart' in model_name:\n",
    "    #get latest checkpoint\n",
    "    def extract_number(f):\n",
    "        s = re.findall(\"\\d+$\",f)\n",
    "        return (int(s[0]) if s else -1,f)\n",
    "    latest_checkpoint = max(os.listdir(model_path),key=extract_number)\n",
    "    print(latest_checkpoint)\n",
    "    model_checkpoint=model_path+'/'+latest_checkpoint\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "    model.to(torch_device)\n",
    "    model.num_parameters() #reduces output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31c8fb-dc96-4a2f-8819-e1000652e987",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea397f4-06ea-42bd-929e-64c4e885a94d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'gpt' in model_name:\n",
    "    from transformers import pipeline, set_seed\n",
    "    generator = pipeline('text-generation',model='gpt2', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443b3fa-5c92-4e98-ab5f-ba97884d4244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'gpt' in model_name:\n",
    "    set_seed(42)\n",
    "    entry_for_manual_testing = 10\n",
    "    for input_text in gyafc['f_r']['text'][:entry_for_manual_testing]: #,dataset['test']['text'][entry_for_manual_testing]\n",
    "        print(input_text)\n",
    "        #task_prefix = \"scientific version: \" # 2 von 10 waren rephrasings/brauchbar\n",
    "        #task_prefix = \"more scientific: \" #0,0!\n",
    "        #task_prefix = \"write scientific text: \" # 4 von 10\n",
    "        #task_prefix = \"write as a scientific sentence: \" # 2 von 10\n",
    "        #task_prefix = \"In scientific language, \" # 4 von 10\n",
    "        task_prefix = \"Write in shakespeare language: \"\n",
    "        res = generator(task_prefix + input_text , max_length = 45, num_return_sequences = 1)\n",
    "        for x in res:\n",
    "            print(x['generated_text'].replace(task_prefix+input_text,'').replace('\\n','').split('.')[0])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05380fc6-730e-4e51-8988-0c30682325cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define output function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8b13c-11e2-43d7-8155-cc16cfabf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(results):\n",
    "    res_string = ('\\n'+model_name+' & '+\n",
    "          str(learning_rate)+' & '+\n",
    "          str(weight_decay)+' & '+\n",
    "          str(round(results['score_bleu']['bleu']*100,2))+' & '+\n",
    "          str(round(results['score_self_bleu']['bleu']*100,2))+' & '+ \n",
    "          str(round(results['score_meteor']['meteor']*100,2))+' & '+\n",
    "          str(round(np.mean(results['score_bertscore']['f1'])*100,2))+' & '+\n",
    "          str(round(math.log(results['perplexity']['mean_perplexity']),3))+' \\\\\\\\\\n')\n",
    "    print(res_string)\n",
    "    f = open(\"eval_results_style.txt\", \"a\")\n",
    "    f.write(res_string)\n",
    "    f.close()\n",
    "    return res_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89892596-5f02-442e-aa3c-a0fb09d8dabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data\n",
    "we only consider the family and relation dataset because model outputs for this are available and therefore allow easy comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5509af9-7e34-41a5-9cd3-ef632b6c901a",
   "metadata": {},
   "source": [
    "## GYAFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf54389-96b1-4e18-9700-35d95157945c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load datasets to test metrics\n",
    "from datasets import load_from_disk, load_dataset, DatasetDict, concatenate_datasets\n",
    "fr_informal = load_dataset(\"text\",data_files=f'{dataset_path}GYAFC_Corpus/Family_Relationships/test/informal')\n",
    "\n",
    "for rewriter in range(4):\n",
    "    load = load_dataset(\"text\",data_files=f'{dataset_path}GYAFC_Corpus/Family_Relationships/test/formal.ref{rewriter}')['train']['text']\n",
    "    fr_informal['train'] = fr_informal['train'].add_column(f'ref{rewriter}', load)\n",
    "\n",
    "model_outputs = ['nmt_baseline','nmt_combined','nmt_copy','pbmt','rule_based']\n",
    "for model_output in model_outputs:\n",
    "    load = load_dataset(\"text\",data_files=f'{dataset_path}GYAFC_Corpus/Family_Relationships/model_outputs/formal.{model_output}')['train']['text']\n",
    "    fr_informal['train'] = fr_informal['train'].add_column(f'{model_output}', load)\n",
    "\n",
    "model_outputs = ['dast-c','dualRL','drlst']\n",
    "for model_output in model_outputs:\n",
    "    load = load_dataset(\"text\",data_files=f'{dataset_path}GYAFC_Corpus/tst_survey/{model_output}')['train']['text']\n",
    "    fr_informal['train'] = fr_informal['train'].add_column(f'{model_output}', load[len(load)-len(fr_informal['train']['text']):])\n",
    "\n",
    "gyafc = DatasetDict({'f_r':fr_informal['train']})\n",
    "gyafc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb46090-6f39-4e99-91d0-eea500417360",
   "metadata": {},
   "outputs": [],
   "source": [
    "entrie = 5\n",
    "[gyafc['f_r'][entrie].get(key) for key in ['ref0','ref1','ref2','ref3']]\n",
    "gyafc['f_r']['drlst'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbaa44-d259-4ab1-bd40-a8c130555533",
   "metadata": {},
   "outputs": [],
   "source": [
    "gyafc['f_r']['ref0'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aef9d4-1514-4446-82e2-884dfc65223b",
   "metadata": {},
   "source": [
    "## Dataset idm or pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a27163-2acb-474e-8004-e7ebafc7138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(f'{dataset_path}style/{dataset_name}')\n",
    "\n",
    "if('idm' in dataset_name):\n",
    "    #idmBuckets is only generated for the test split! Therefore, the renaming is also limited!\n",
    "    if('idm-small' in dataset_name): \n",
    "        dataset['test'] = dataset['test'].rename_column('idm','para-1-1')\n",
    "    else:\n",
    "        #original 'idm' if we do buckets use idmBucket \n",
    "        dataset['test'] = dataset['test'].rename_column('idmBucket','para-1-1')\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "dataset['test'] = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8d4fd-85d2-4e3b-89c8-65a18ae944ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da762682-7b38-4f83-8814-b5ba65f80d6f",
   "metadata": {},
   "source": [
    "# Load metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4080294-a394-4bf3-88b3-f0985d0da8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from datasets import load_metric\n",
    "#https://huggingface.co/metrics\n",
    "\n",
    "def calculate_all_metrics(model_input,model_output,model_reference):\n",
    "    metric_bleu = load_metric(\"bleu\") \n",
    "    metric_self_bleu = load_metric(\"bleu\") \n",
    "    metric_rouge = load_metric(\"rouge\") \n",
    "    metric_meteor = load_metric(\"meteor\") \n",
    "    metric_bertscore = load_metric(\"bertscore\")\n",
    "    metric_ppl = load_metric(\"perplexity\") \n",
    "    \n",
    "    for entry in range(len(model_input)):\n",
    "        x_in = model_input[entry].lower().split(' ')\n",
    "        x_out = model_output[entry].lower().split(' ')\n",
    "        x_ref = model_reference[entry]\n",
    "        x_ref = [x.lower().split(' ') for x in x_ref]\n",
    "        \n",
    "        x_out_bert = model_output[entry][:512].lower()\n",
    "        x_ref_bert = model_reference[entry][:512]\n",
    "        x_ref_bert = [x.lower() for x in x_ref_bert]  #split wurde hier weggelassen!\n",
    "\n",
    "        metric_bleu.add_batch(predictions = [x_out], references= [x_ref])\n",
    "        metric_self_bleu.add_batch(predictions = [x_out], references= [[x_in]]) \n",
    "        metric_meteor.add_batch(predictions = [x_out], references= [x_ref])\n",
    "        metric_bertscore.add_batch(predictions = [x_out_bert], references= [[x_ref_bert]],)\n",
    "\n",
    "    return {'score_bleu' : metric_bleu.compute(),\n",
    "    'score_self_bleu' : metric_self_bleu.compute(),\n",
    "    'score_meteor' : metric_meteor.compute(),\n",
    "    'score_bertscore' : metric_bertscore.compute(model_type='allenai/scibert_scivocab_uncased',),\n",
    "    'perplexity': metric_ppl.compute(input_texts = [x[:512].lower().split(' ') for x in model_output if (len(x.lower().split(' '))>2) ], model_id='allenai/scibert_scivocab_uncased',add_start_token=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8627c7ea-1325-4792-8c77-9c3f885c2b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Human evaluation\n",
    "\n",
    "Checklist:\n",
    "* Is the sentence a correct english sentence?\n",
    "    * Are words meaninglessly repeated?\n",
    "* Is the sentence meaningful?\n",
    "* Is the original meaning preserved?\n",
    "    * loss of information \n",
    "    * addition of unrelated information\n",
    "* Is it more scientific?\n",
    "    * scientific words/wording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9852a9a-9dca-4a36-8e2c-a28941c2c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 5 # less word repetition if larger 1 \n",
    "num_return_sequences = 1\n",
    "entry_for_manual_testing = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f22e93-db7a-4cb8-bd04-f58cf13fc742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for input_text in [gyafc['f_r']['text'][entry_for_manual_testing],dataset['test']['text'][entry_for_manual_testing],dataset['test']['para-1-1'][entry_for_manual_testing]]:\n",
    "    #input_text = '>> My dog ate my homework'\n",
    "    print(input_text)\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=100, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=100,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5) #TODO test output_scores = True\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    print(tgt_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f8f7a-c73f-4cf2-b5a6-e093b1bee0c0",
   "metadata": {},
   "source": [
    "# Apply metric to test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccead7-7256-4837-8547-c6f45dbfb483",
   "metadata": {},
   "source": [
    "### T5 or BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40594f-b659-44bb-a3c0-faa7d37a07a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_text= []\n",
    "if (('t5' in model_name or 'bart' in model_name):\n",
    "    and not os.path.exists(f'{output_path}style/{model_output_name}')):\n",
    "    for x in tqdm(range(0, len(dataset['test']['text']), batch_size)):\n",
    "        subset=dataset['test']['para-1-1'][x:x+batch_size]\n",
    "        batch = tokenizer(subset,truncation=True,padding='max_length',max_length=100, return_tensors=\"pt\").to(torch_device)\n",
    "        translated = model.generate(**batch,max_length=100,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "        tgt_text.extend(tokenizer.batch_decode(translated, skip_special_tokens=True))\n",
    "\n",
    "    dataset['test']=dataset['test'].add_column(f'{model_output_name}', tgt_text)\n",
    "\n",
    "    #save results really redundand!!!\n",
    "    dataset.save_to_disk(f'{output_path}style/{model_output_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1be1d-371c-4a4a-8c83-c70d406ec07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results if save task was performed before\n",
    "dataset = dataset.load_from_disk(f'{output_path}style/{model_output_name}')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1bd18-dccc-4d3e-a40e-7ed45e2f8f52",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbc464-eb64-4c55-b107-8cb31e8d24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_text= []\n",
    "if 'gpt' in model_name:\n",
    "    for input_text in tqdm(dataset['test']['text']):\n",
    "        #task_prefix = \"Write in shakespeare language:  \n",
    "        #task_prefix = \"write scientific text: \"\n",
    "        task_prefix = \"In scientific language, \"\n",
    "        res = generator(task_prefix + input_text , max_length = 45, num_return_sequences = 1)\n",
    "        tgt_text.append(res[0]['generated_text'].replace(task_prefix+input_text,'').replace('\\n','').split('.')[0])\n",
    "\n",
    "    dataset['test']= dataset['test'].add_column(f'gpt2 + {task_prefix}', tgt_text)\n",
    "    model_output_name = f'gpt2 + {task_prefix}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21362c-e261-4335-bc89-9cf8e5788042",
   "metadata": {},
   "source": [
    "### identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2fb33-4ed3-40d2-8d4f-c08e1e85b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'identity' in model_name: \n",
    "    model_output_name = 'para-1-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3378a-dd4b-4ea5-8ee5-5b2be0cdba5c",
   "metadata": {},
   "source": [
    "### with buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb03a93-16d6-45d9-a556-0ee50862ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "werScores = dataset.load_from_disk(f'{dataset_path}style/para-1-1')['test']\n",
    "werScores = werScores['wer-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8d73d-0589-49ec-b548-e4f20d63edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pegasus dataset\n",
    "if 'para-1-1' in dataset_name:\n",
    "    buckets=[0,1,2,3,4,5]#[0,0.1,0.2,0.3,0.4,0.5]\n",
    "    bins = pd.qcut(werScores,6,labels=buckets)\n",
    "#IDM dataset\n",
    "elif 'idm' in dataset_name:\n",
    "    buckets=[0,0.1,0.2,0.3,0.4,0.5]\n",
    "    bins = dataset['test']['bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30c303-b7d5-464c-a9c7-08ac2ff7dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bucket= []\n",
    "for bucket in buckets:\n",
    "    idx_buckets = [idx for idx, bbin in enumerate(bins) if (bucket == bbin)]\n",
    "    dataset_bucket.append(dataset['test'].select(idx_buckets))\n",
    "\n",
    "dataset_bucket[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15d384-8a25-4cf4-9084-9f863fea5dfa",
   "metadata": {},
   "source": [
    "#use the entire test set e.g. this was used to calculate the overall metrics during hyperparameter tuning\n",
    "dataset_bucket = []\n",
    "dataset_bucket.append(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bd7f9-0ac2-475d-b92f-885d6a0a2755",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Information on how the different buckets look\n",
    "selector = 0 # selects a bucket\n",
    "sentence_idx=3\n",
    "print('Config: ',selector,sentence_idx,model_output_name)\n",
    "print('Gold Standard:')\n",
    "print(dataset_bucket[selector]['text'][sentence_idx])\n",
    "print('\\nInput')\n",
    "print(dataset_bucket[selector]['para-1-1'][sentence_idx])\n",
    "#print(dataset_bucket[selector]['idm'][sentence_idx])\n",
    "print('\\nModel output')\n",
    "print(dataset_bucket[selector][model_output_name][sentence_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d77a6-72da-474c-a96a-77783b8e71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_idx(idx):\n",
    "    model_checkpoint = [\"google/t5-v1_1-small\",\"google/t5-v1_1-base\",\"google/t5-v1_1-large\",\"facebook/bart-base\"][idx]\n",
    "    model_name = model_checkpoint.split(\"/\")[-1]\n",
    "    dataset_name = 'idm'\n",
    "\n",
    "    model_output_name =f'{model_name}-finetuned-{dataset_name}-lr-{learning_rate}-wd-{weight_decay}'\n",
    "    \n",
    "    try:\n",
    "        dataset = load_from_disk(f'{output_path}style/{model_output_name}')\n",
    "        \n",
    "        #Pegasus dataset\n",
    "        if 'para-1-1' in dataset_name:\n",
    "            buckets=[0,1,2,3,4,5]#[0,0.1,0.2,0.3,0.4,0.5]\n",
    "            bins = pd.qcut(werScores,6,labels=buckets)\n",
    "        #IDM dataset\n",
    "        elif 'idm' in dataset_name:\n",
    "            buckets=[0,0.1,0.2,0.3,0.4,0.5]\n",
    "            bins = dataset['test']['bucket']\n",
    "    \n",
    "        dataset_bucket= []\n",
    "        for bucket in buckets:\n",
    "            idx_buckets = [idx for idx, bbin in enumerate(bins) if (bucket == bbin)]\n",
    "            dataset_bucket.append(dataset['test'].select(idx_buckets))\n",
    "        \n",
    "        \n",
    "        if(idx==0):\n",
    "            print(f'\\multirow{{6}}{{*}}{{{selector}}}')\n",
    "            print(' & gold standard & ',dataset_bucket[selector]['text'][sentence_idx+i],'\\\\\\\\')\n",
    "            print(' & input sentence & ',dataset_bucket[selector]['para-1-1'][sentence_idx+i],'\\\\\\\\')\n",
    "        print('&',model_name, '&',dataset_bucket[selector][model_output_name][sentence_idx+i],'\\\\\\\\')\n",
    "    \n",
    "    except (FileNotFoundError, KeyError) as error:\n",
    "        print('\\nERROR\\n',model_name)\n",
    "    return dataset_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c2b7d-dc1b-4ef4-bc25-f43b68a9aa0e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Information on how the different buckets look\n",
    "selector = 5 # selects a bucket\n",
    "#pegasus bucket: 0 sentence: 80, 110, 1123\n",
    "#pegasus bucket: 1 sentence: 4\n",
    "#pegasus bucket: 2-5 sentence: 3\n",
    "\n",
    "#idm bucket: 0 sentence: 0, 1002?\n",
    "#idm bucket: 1 sentence: 1001\n",
    "#idm bucket: 2 sentence: \n",
    "#idm bucket: 3 sentence: 1002\n",
    "#idm bucket: 4 sentence: 1001\n",
    "#idm bucket: 5 sentence: 1051-> good, 1298, change meaning: 1323, 1440\n",
    "\n",
    "sentence_idx=1001\n",
    "for selector in range(0,6):\n",
    "    for i in range(0,1000):\n",
    "        if(len(dataset_bucket[selector]['text'][sentence_idx+i])<115):\n",
    "            for idx in range(0,4):\n",
    "                output_idx(idx)\n",
    "            print('\\midrule')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377dacb4-3ccc-481e-b0bd-2608edf1767e",
   "metadata": {},
   "source": [
    "### calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8cbd68-f6ae-4401-a438-fceca5a18914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_test_set= f\"\\n\\nRESULTS\\n\"\n",
    "for idx, bucket in enumerate(dataset_bucket):\n",
    "    print('Bucket: ',idx)\n",
    "    model_input=bucket['para-1-1']#[String]\n",
    "    model_output=bucket[model_output_name]#[String]\n",
    "    model_reference=[[x] for x in bucket['text']]#[[String]]\n",
    "    \n",
    "    results = calculate_all_metrics(model_input,model_output,model_reference)\n",
    "    resAsString = print_output(results)\n",
    "    results_test_set=f'{results_test_set} Bucket:{idx}\\n {resAsString}\\n'\n",
    "print(results_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c8ca3-b03d-4ddf-aeab-f7650b30ac89",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Generate gyafc output with our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54725c9-79d3-434e-9019-43fd74d7dcd5",
   "metadata": {},
   "source": [
    "### T5 or BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e05426-da21-410d-bbf8-c6cf38a3a6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tgt_text= []\n",
    "#max_length=60 for Pegasus \n",
    "num_beams=5\n",
    "for x in tqdm(range(0, len(gyafc['f_r']['text']), batch_size)):\n",
    "    subset=gyafc['f_r']['text'][x:x+batch_size]\n",
    "    batch = tokenizer(subset,truncation=True,padding='max_length',max_length=100, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=100,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text.extend(tokenizer.batch_decode(translated, skip_special_tokens=True))\n",
    "\n",
    "gyafc['f_r']=gyafc['f_r'].add_column(f'{model_output_name}', tgt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec18642-242d-4481-9cf1-12e0ce56e75f",
   "metadata": {},
   "source": [
    "### self-idm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02c351-19c7-490f-b993-4c34f3c01671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self-idm\n",
    "if identity in model_name:\n",
    "    tgt_text = modify(gyafc['f_r']['text'])\n",
    "    gyafc['f_r']=gyafc['f_r'].add_column(f'{model_output_name}', tgt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14d1dd-9d2e-4ae9-a2f4-d507c0b5ca57",
   "metadata": {},
   "source": [
    "### GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08659063-c8a8-4785-b8af-d9b89f0cb45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_text= []\n",
    "from tqdm import tqdm\n",
    "if 'gpt' in model_name:\n",
    "    for input_text in tqdm(gyafc['f_r']['text']):\n",
    "        #subset=gyafc['f_r']['text'][x:x+batch_size]\n",
    "        #task_prefix = \"Write in shakespeare language:  \n",
    "        #task_prefix = \"write scientific text: \"\n",
    "        task_prefix = \"In scientific language, \"\n",
    "        res = generator(task_prefix + input_text , max_length = 45, num_return_sequences = 1)\n",
    "        tgt_text.append(res[0]['generated_text'].replace(task_prefix+input_text,'').replace('\\n','').split('.')[0])\n",
    "        \n",
    "        #translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "        #tgt_text.extend(tokenizer.batch_decode(translated, skip_special_tokens=True))\n",
    "\n",
    "    gyafc['f_r']=gyafc['f_r'].add_column(f'gpt2 + {task_prefix}', tgt_text)\n",
    "    model_output_name = f'gpt2 + {task_prefix}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d945c-4266-4ec8-b3ca-5f479481c579",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply metric to gyafc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030066a-06bd-4b70-802a-0e24cbd9b6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "model_input=gyafc['f_r']['text']#[String]\n",
    "model_reference=[[x.get(key)  for key in ['ref0','ref1','ref2','ref3']] for x in gyafc['f_r']] #[[String]]\n",
    "\n",
    "#allows to get the metrics for the reference datasets\n",
    "model_outputs = [model_output_name]#['text','nmt_combined','rule_based','dualRL','drlst','dast-c']#[model_output_name]#['t5finetuned-1-1','pegasus-1-1','nmt_combined','rule_based','dast-c','dualRL','drlst','text']#[]#['text']\n",
    "for model_output_name in model_outputs:\n",
    "    model_output=gyafc['f_r'][model_output_name]#[String]\n",
    "    results = calculate_all_metrics(model_input,model_output,model_reference)\n",
    "    results_gyafc_set=results # to access results later for pretty and combined print\n",
    "    print_output(results_gyafc_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793a238-674e-484d-aa9d-1e80cae90b24",
   "metadata": {},
   "source": [
    "# Combined output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48364307-7842-4c18-929f-497f06a67a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test set:')\n",
    "print(results_test_set)\n",
    "print()\n",
    "print('gyafc dataset:')\n",
    "print_output(results_gyafc_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
