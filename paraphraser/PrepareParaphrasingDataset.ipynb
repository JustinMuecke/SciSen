{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9271fc6c-5862-4ab6-9c1c-ba7718f64884",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c643a17-ada3-43a3-a8f0-6a3c31c06c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/media/nvme3n1/proj_scisen/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8dc83-9a71-4610-af76-62039becf50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f918f-7f6a-4e5b-93c5-b77302385036",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Files\n",
    "\n",
    "use datasets to load custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c46b7-d6d1-4522-a2c4-aa96f799cef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasetSci = load_dataset(\"json\",data_files=f'{path}datasets/SciSections_sentences.jsonl')\n",
    "\n",
    "def filter_as_a(x):\n",
    "    if x['rank']=='as':\n",
    "        return True\n",
    "    if x['rank']=='a':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "datasetSci = datasetSci.filter(function=filter_as_a)\n",
    "print('unique sentences: ',sum([len(x) for x in np.unique(datasetSci['train']['sentences'])]))\n",
    "my_dict = {\"text\": np.unique([l for x in datasetSci['train']['sentences'] for l in x])}\n",
    "datasetSci = Dataset.from_dict(my_dict)\n",
    "datasetSci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed5377d-724f-4d1e-9748-a56226520abd",
   "metadata": {},
   "source": [
    "# Overwrite existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef001b8b-49aa-4733-970c-0f452e3b685f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#work around because already open dataset can't be written\n",
    "def save_dataset(dataset,name):\n",
    "    from datasets import load_from_disk\n",
    "    dataset.save_to_disk(f'{path}datasets/style/paraphrases_temp')\n",
    "    dataset = load_from_disk(f'{path}datasets/style/paraphrases_temp')\n",
    "    dataset.save_to_disk(f'{path}datasets/style/{name}')\n",
    "    dataset = load_from_disk(f'{path}datasets/style/{name}')\n",
    "    #%rm -r f'{path}datasets/style/paraphrases_temp #NOT working "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a175698-b4b4-400f-8246-b4d779c02dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create huggingface dataset\n",
    "\n",
    "concatenate all the small parts into one big dataset\n",
    "and split it according to train validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa2a10-3eef-4840-b726-d5b106f63cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "from datasets import DatasetDict\n",
    "import math\n",
    "\n",
    "dataset = concatenate_datasets([datasetSci])\n",
    "#split data and add it to a dictionary\n",
    "#set fraction to 1 to use the entire dataset otherwise e.g. 0.1\n",
    "fraction= 1\n",
    "dataset = DatasetDict({'train':dataset.shuffle(seed=42).select(range(math.floor(len(dataset)*0.7*fraction))),\n",
    "                       'val':dataset.shuffle(seed=42).select(range(math.floor(len(dataset)*0.7*fraction+1),math.floor(len(dataset)*0.9*fraction))),\n",
    "                       'test':dataset.shuffle(seed=42).select(range(math.floor(len(dataset)*0.9*fraction+1),math.floor(len(dataset)*fraction)))\n",
    "                      })\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5d0fd-81a7-4b61-8b27-b1541eddc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample output\n",
    "dataset['train']['text'][:1]\n",
    "save_dataset(dataset,'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14eb52-9e46-4767-94f8-7e8af4ada487",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create parallel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46f68e-faa4-4e2c-a069-94bf74de6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset from checkpoint - after split with modifications etc.\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(f'{path}datasets/style/dataset')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee2b57-55c5-494d-8737-1ba8a8d7a4b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## with Pegasus Paraphraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a44647-6946-44b9-9518-cbd93bfa48eb",
   "metadata": {},
   "source": [
    "load model and create method for sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7f79c-8fe4-4d0d-88fb-2705cf73fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(torch_device)\n",
    "\n",
    "#create sample outputs for single sentences\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943783c-4cef-442b-a2d7-5754ea52afa6",
   "metadata": {},
   "source": [
    "create sample outputs for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6f12e3-381f-4b9e-bd22-80b860a05164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "num_beams = 1\n",
    "num_return_sequences = 1\n",
    "batchsize = 50#900\n",
    "dataset_name = 'para-1-1'\n",
    "column_name= 'para-1-1'\n",
    "\n",
    "for splits in dataset.keys(): #['train', 'val', 'test']\n",
    "    tgt_text= []\n",
    "    for x in tqdm(range(0, len(dataset[splits]['text']), batchsize)):\n",
    "        subset=dataset[splits]['text'][x:x+batchsize]\n",
    "        batch = tokenizer(subset,truncation=True,padding='max_length',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "        translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "        tgt_text.extend(tokenizer.batch_decode(translated, skip_special_tokens=True))\n",
    "\n",
    "    dataset[splits]=dataset[splits].add_column(column_name, tgt_text)\n",
    "\n",
    "    dataset.save_to_disk(f'{path}datasets/style/{dataset_name}')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff31932-177d-4e42-bce6-eca984b12b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tgt_text= []\n",
    "batch_size=2\n",
    "num_beams = 1 #4 als goldene Mitte?\n",
    "num_return_sequences = 1 #4 als goldene Mitte?\n",
    "\n",
    "for x in tqdm(range(0, len(dataset['test']['text']), batch_size)):\n",
    "    subset=dataset['test']['text'][x:x+batch_size]\n",
    "    batch = tokenizer(subset,truncation=True,padding='max_length',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text.extend([tokenizer.batch_decode(translated, skip_special_tokens=True)])\n",
    "\n",
    "dataset['test']=dataset['test'].add_column(f'{model_name}-finetuned-{dataset_name}-lr-{learning_rate}-wd-{weight_decay}', tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78041d3-ba6e-4d3f-b322-e36cda3e0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b37bd3-f826-4b09-858e-8066b17cee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1382f5e-14ea-477a-8b2b-e53bed4202ab",
   "metadata": {},
   "source": [
    "## add [WER](https://huggingface.co/spaces/evaluate-metric/wer) distanz for buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3f85e-5fda-4996-ab0a-47e64ab311ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(f'{path}datasets/style/para-1-1')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550359a-c70f-447e-b02c-405c656bd45f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "wer = load(\"wer\")\n",
    "column_name = \"wer-score\"\n",
    "    \n",
    "tgt_text= []\n",
    "for x in tqdm(range(0, len(dataset['test']['text']))):\n",
    "    #for a single element: predictions and reference are not interchangable\n",
    "    predictions= [dataset['test']['para-1-1'][x]]\n",
    "    references = [dataset['test']['text'][x]]\n",
    "    tgt_text.append(wer.compute(predictions=predictions, references=references))\n",
    "                    \n",
    "dataset['test']=dataset['test'].add_column(column_name, tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145b3f4-2434-4625-a5ea-f30c990acb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b85dc-c3a8-4028-9b1e-b77e62d7203a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tgt_text[:10] # nicht lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d59b1-3fbe-4b4a-8d57-034a8f8817e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(dataset,'para-1-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebee106-2095-4c5d-a20a-592da6311589",
   "metadata": {
    "tags": []
   },
   "source": [
    "## IDM (Insert Delete Modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045e4d3-bc95-4e78-8665-8cb317cbbf93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased',device=0)#.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363fe6e-88d4-4f1d-9f4a-2164a18e2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_replace(sentences,replace=1,add_mask=True):#example: sentences=['String sentece two.','I am a sentence.']\n",
    "    #create batch of sentence to allow batched unmasking if required \n",
    "    new_sentences=[]\n",
    "    for sentence in sentences:\n",
    "        #unmask only accepts one MASK token, this is a problem when the original sentence already contains one \n",
    "        #handle [MASK] like any other word\n",
    "        sentence = sentence.replace('[MASK]','MASK')\n",
    "            \n",
    "        split = sentence.split(' ')\n",
    "        \n",
    "        selected_word = random.randrange(0, len(split))\n",
    "\n",
    "        #find words larger than three characters as long as such words exits and only if we want to replace/delete a word\n",
    "        while (len(split[selected_word]) < 4) and (replace > 0) and (max([len(x) for x in split])>3):\n",
    "            selected_word = random.randrange(0, len(split))\n",
    "            \n",
    "        split1 = split[:selected_word]\n",
    "        split2 = split[selected_word+replace:]\n",
    "\n",
    "        if add_mask:\n",
    "            sentence = ' '.join(split1 +['[MASK]']+ split2) # unmasker fehlt\n",
    "        else:\n",
    "            sentence = ' '.join(split1 + split2)\n",
    "\n",
    "        new_sentences.append(sentence)\n",
    "          \n",
    "    if add_mask:\n",
    "        #join here -> later only unmask or return\n",
    "        my_dict = {\"text\": new_sentences}\n",
    "        dataset_sentences = Dataset.from_dict(my_dict)\n",
    "        \n",
    "        out_sentences=[]\n",
    "        unmasked_sentences = unmasker(KeyDataset(dataset_sentences, \"text\"), batch_size=1000,top_k=3,)\n",
    "        for idx, sentence in enumerate(unmasked_sentences):\n",
    "            #ensure that the sentence changed\n",
    "            if (sentences[idx]!=sentence[0]['sequence']) and ('\\'' not in sentence[0]['sequence']):\n",
    "                out_sentences.extend([sentence[0]['sequence']])\n",
    "            elif (sentences[idx]!=sentence[1]['sequence']) and ('\\'' not in sentence[1]['sequence']):\n",
    "                out_sentences.extend([sentence[1]['sequence']])\n",
    "            else:\n",
    "                out_sentences.extend([sentence[2]['sequence']])\n",
    "        return out_sentences\n",
    "    else:\n",
    "        return [out for out in new_sentences]\n",
    "\n",
    "modify_replace(np.array([\"We stated, justified, and\",'String sentece two.','I am a sentence.','Ich',]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132e4ec-409d-42cd-886f-0d0b50b5e453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test to check if \n",
    "def test_function(example_sentence,modify_function,changes_to_length=0,repetition=100):\n",
    "    for i in range(0,repetition):#debug because size changes created an error\n",
    "        res = modify_function(np.array(example_sentence))\n",
    "        if len(res[0].split(' ')) != len(example_sentence[0].split(' '))+changes_to_length:\n",
    "            print('Not intended modification: ')\n",
    "            print(example_sentence,len(example_sentence[0].split(' ')))\n",
    "            print(res,len(res[0].split(' ')))\n",
    "            print()\n",
    "\n",
    "example= ['We stated, justified, and']\n",
    "example_mask= ['We stated, justified, [MASK] [MASK] and']\n",
    "\n",
    "test_function(example,modify_replace)\n",
    "test_function(example,modify_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42effe4-0b79-4ccd-8e2c-b50371df2e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modify_delete(sentence):\n",
    "    return modify_replace(sentence,add_mask=False)\n",
    "\n",
    "test_function(example,modify_delete,-1)\n",
    "test_function(example_mask,modify_delete,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69f5dd-b78f-44b6-b889-3edba09a69c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_insert(sentence):\n",
    "    return modify_replace(sentence,replace = 0)\n",
    "\n",
    "#results in one additional 'word' if there is a trailing space ' ' doesn't change actual sentences but messes with the test\n",
    "test_function(example,modify_insert,1)\n",
    "\n",
    "#also check multiple mask tokens in sentence\n",
    "test_function(example_mask,modify_insert,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca4810-154e-4b6a-b48d-c4d13da1a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def modify(sentences):#=['String sentece two.','I am a sentence.']    \n",
    "    sentences = np.array(sentences)\n",
    "\n",
    "    max_changes_all = math.ceil(len(sentences[0].split(' '))/2)\n",
    "    buckets = [(random.randrange(0,6)/10 ,len(x.split(' '))) for x in sentences]\n",
    "    changes_bucket = [math.floor(length/2)*changes for changes, length in buckets]    \n",
    "        \n",
    "    for changes in range(0,max_changes_all):\n",
    "        #change all senteces that have not the required amount of changes for their bucket\n",
    "        selected_sentences = [bucket > changes for bucket in changes_bucket]\n",
    "        #print(selected_sentences)#shows True/False of sentence selection\n",
    "        r = random.randrange(0,3)\n",
    "        if r==0:\n",
    "            sentences[selected_sentences] =  modify_replace(sentences[selected_sentences])\n",
    "        elif r==1:\n",
    "            sentences[selected_sentences] =  modify_delete(sentences[selected_sentences])\n",
    "        else:\n",
    "            sentences[selected_sentences] =  modify_insert(sentences[selected_sentences])\n",
    "    return sentences, buckets\n",
    "\n",
    "modify(np.array([\"We stated, justified, and We stated, justified, and\",'String sentece two on one.','I am a single sentence.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9dca0-e2f5-493c-b08b-4b6258f5756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from datasets import Dataset\n",
    "dataset= load_from_disk(f'{path}datasets/style/idm')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48035209-a101-47bc-9975-06275a410ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sort sentences by length for optimization \n",
    "#perform the same maximum of changes on sentences with the same length\n",
    "for splits in dataset.keys(): #['train', 'val', 'test']\n",
    "    dataset[splits] = dataset[splits].add_column('length',[len(sentence.split(' ')) for sentence in dataset[splits]['text']])\n",
    "    dataset[splits] = dataset[splits].sort('length',reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46d872-5492-4527-9746-80e41885bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['text'][-30000:-29997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4b214-576d-43ba-8d2d-1d1ae7fd2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "modify(dataset['test']['text'][-30000:-29997])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bb682-5740-4d78-906b-3d250dee251c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "column_name = 'idmBucket'\n",
    "column_name_bucket= 'bucket'\n",
    "dataset_name = 'idm'\n",
    "batchsize = 300 #1000~27GB could be improved as this is only required for the longest sentences\n",
    "\n",
    "for splits in ['test']:#dataset.keys(): #['train', 'val', 'test']\n",
    "    tgt_text= []\n",
    "    tgt_bucket = []\n",
    "    #create batches of sentences -> workaround as split is not defined on dataset.map\n",
    "    for x in tqdm(range(0, len(dataset[splits]['text']), batchsize)):\n",
    "        subset=dataset[splits]['text'][x:x+batchsize]\n",
    "        text, bucket = modify(subset)\n",
    "        tgt_text.extend(text)\n",
    "        tgt_bucket.extend([x[0] for x in bucket])\n",
    "    \n",
    "    dataset[splits]=dataset[splits].add_column(column_name, tgt_text)\n",
    "    dataset[splits]=dataset[splits].add_column(column_name_bucket, tgt_bucket)\n",
    "\n",
    "    save_dataset(dataset,dataset_name)#risky operation, as the old dataset is overwritten\n",
    "    #dataset.save_to_disk(f'{path}datasets/style/{dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366a783-a689-4ec4-a148-4b2b284b89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede9adb-9c44-4eae-96ad-953f0713756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['bucket'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ad614-1d81-4c67-be44-f72fe60471a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test']['idmBucket'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265da33-f280-4022-b593-55c2d166d129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
