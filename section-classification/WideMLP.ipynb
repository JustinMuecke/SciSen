{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28451d8d-93a4-4c75-9b35-ee2905c878c7",
   "metadata": {},
   "source": [
    "# Wide Multi-Level Perceptron (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037111e-6cdf-45da-b673-c66fe38e355d",
   "metadata": {},
   "source": [
    "WideMLP from Diera et al.(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97795f6c-8ecf-42fd-bd5e-2997c7323f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (AdamW, AutoTokenizer, get_linear_schedule_with_warmup)\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from collections import Counter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486f5e0-1169-478e-84e4-a6467585d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910365f1-faef-4ecf-93e3-c84f364e576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a9800-8688-4687-8577-74d157295a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "WANDB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc7e9d-4002-4908-9802-3ffd2b75bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"WideMLP_2s_32_0.1\" #for filenames\n",
    "wandb_id = \"WideMLP_2s_32_0.1_0.2\"\n",
    "wandb_project = \"section-clf-multisen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0710ca-50cd-40f9-baef-4fe49fdcc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "tokenizer_name = \"bert-base-uncased\"\n",
    "\n",
    "INPUT_PATH='/media/nvme3n1/proj_scisen/datasets/SciSections_sentences.jsonl'\n",
    "MODEL_PATH='/media/nvme3n1/proj_scisen/models/MLSC/'\n",
    "RESULTS_PATH='/media/nvme3n1/proj_scisen/results/MLSC/'\n",
    "\n",
    "LAMBDA=0.2\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=100\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339d97d-9c43-470f-a76a-bb1240ea245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_appendices = False\n",
    "included_conferences = ['as','a','b','c']\n",
    "context_width = 2\n",
    "\n",
    "test_splits = [['as','a','b','c']] # [ [['as','a','b','c']] , [['as'],['a'],['b','c']] ] test data split by conference rank\n",
    "\n",
    "validate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6aafce-b1c8-47cf-a018-a0d304415830",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9ffb6-cda7-4feb-bc5f-28885c8ba226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder; move/delete later TODO\n",
    "\n",
    "#embedding = None\n",
    "#do_truncate = False\n",
    "#max_length = None # we only compute dataset stats including length, so NOT truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba5765-18fc-4737-a673-ca2bf6e62334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the decice for GPU usage\n",
    "torch_device = 'cuda:3' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01214d33-19d8-412a-8e6b-cbb732f28966",
   "metadata": {},
   "source": [
    "### set up WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fe0b8-2365-474e-a8bd-2aee56dd7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    wandb.init(project=wandb_project, name = wandb_id, config={\"epochs\": EPOCHS, \"context_width\": context_width, \"validation\": validate, \"batch_size\": BATCH_SIZE, \"learning_rate\": LEARNING_RATE, \"lambdas\": LAMBDA, \"trainingdata\":\"full\", \"conferences\": \"asabc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc89ca-f92f-457f-9be0-263f6b1f750b",
   "metadata": {},
   "source": [
    "## load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9b733-1941-4ad7-9b73-d2f8cb6d8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphList = list()\n",
    "with open(INPUT_PATH) as f:\n",
    "    for paragraph in f:\n",
    "        paragraphDict = json.loads(paragraph)\n",
    "        paragraphList.append(paragraphDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010a930-f93c-4eac-aad1-9adc0acfb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(sentences, sentence_idx, k_context):   \n",
    "### method of choosing sentence context ###\n",
    "#use only consecutive sentences within one paragraph (no [removed] tokens or end of paragraph):\n",
    "#k=1: no context, k=2: predecessor+sentence, k = 3: predecessor+sentence+successor, k=2n: n predecessors+S+(n-1) sucessors, k=2n+1: n predecessors+S+n sucessors\n",
    "#kick if context is empty or not of desired length or contains [removed] sentence\n",
    "    k=k_context\n",
    "    context = []\n",
    "    start = sentence_idx - k//2\n",
    "    stop = sentence_idx + k//2 if k%2 == 0 else sentence_idx + k//2 + 1\n",
    "    if start >= 0 and stop <= len(sentences):\n",
    "        context = sentences[start : stop]    \n",
    "    else:\n",
    "        return \"\"\n",
    "    if \"[removed]\" in context:\n",
    "        return \"\"\n",
    "    return \" \".join([str(item) for item in context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff33e6-fc60-455a-a6b9-e906ba8f56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputList = []\n",
    "for par in tqdm(paragraphList):\n",
    "    if par['section_category'] == ['appendix'] and not include_appendices:\n",
    "        continue\n",
    "    if not par['rank'] in included_conferences:\n",
    "        continue\n",
    "    for idx, sentence in enumerate(par['sentences']):\n",
    "        if not (sentence == \"[removed]\"):\n",
    "            #text_target = sentence #add context here\n",
    "            #text_context = context(par['sentences'],idx,context_width) \n",
    "            text_target = context(par['sentences'],idx,context_width) \n",
    "            #inputList.append({**{'text_target': text_target, 'text_context': text_context}, **label})\n",
    "            inputList.append({**{'text': text_target}, 'labels':par['section_category'], 'rank': par['rank']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263c710-bcef-4b94-989b-caa7e515f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.DataFrame(inputList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68128988-6328-4b6b-9f3f-265f69ef13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input=df_input.mask(df_input == '')\n",
    "df_input = df_input[~df_input['text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b3201-d6c4-4542-bd8e-fafb51461ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller datasets for (pre-)experiments\n",
    "N = len(df_input)\n",
    "\n",
    "#random 10 %\n",
    "#df_input = df_input.sample(n=ceil(N*0.1), random_state = 42)\n",
    "#df_input = df_input.sample(n=10000, random_state = 42)\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da102b7-33e3-4fa1-ba15-fecacdc3489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split: 70/20/10\n",
    "train_size = 0.7\n",
    "valid_size = 0.2\n",
    "\n",
    "#split into train, test:\n",
    "trainvalid_df=df_input.sample(frac= train_size + valid_size ,random_state=200)\n",
    "test_dataset=df_input.drop(trainvalid_df.index).reset_index(drop=True)\n",
    "trainvalid_df = trainvalid_df.reset_index(drop=True)\n",
    "\n",
    "#split test set by conference rank\n",
    "test_datasets = dict()\n",
    "for split in test_splits:\n",
    "    test_datasets[str(split)]=test_dataset[test_dataset['rank'].isin(split)].reset_index(drop=True)\n",
    "\n",
    "#split into train, validation:\n",
    "train_dataset=trainvalid_df.sample(frac= train_size/(train_size + valid_size) ,random_state=200)\n",
    "valid_dataset=trainvalid_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df_input.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"FULL TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "for split in test_splits:\n",
    "    print(\"TEST Dataset ranks {}: {}\".format(str(split), test_datasets[str(split)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de76f1-1759-4f51-a978-8a5a29bdab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_dataset['text'].values.tolist(), dtype=object)\n",
    "valid_data = np.array(valid_dataset['text'].values.tolist(), dtype=object)\n",
    "raw_documents = np.append(train_data, valid_data)\n",
    "test_data_dict = dict()\n",
    "for split in test_splits:\n",
    "    test_data_dict[str(split)] = np.array(test_datasets[str(split)]['text'].values.tolist(), dtype=object)\n",
    "    raw_documents = np.append(raw_documents, test_data_dict[str(split)])\n",
    "N = len(raw_documents)\n",
    "\n",
    "print(\"Loading document metadata...\")\n",
    "train_labels = np.array(train_dataset['labels'].values.tolist(), dtype=object)\n",
    "test_labels_dict = dict()\n",
    "valid_labels = np.array(valid_dataset['labels'].values.tolist(), dtype=object)\n",
    "labels = np.append(train_labels.data, valid_labels.data)\n",
    "for split in test_splits:\n",
    "    test_labels_dict[str(split)] = np.array(test_datasets[str(split)]['labels'].values.tolist(), dtype=object)\n",
    "    labels = np.append(labels, test_labels_dict[str(split)].data)\n",
    "labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "unique_labels = list(Counter(labels).keys())\n",
    "\n",
    "labels_in_order = ['introduction',\n",
    "        'related work',\n",
    "        'method',\n",
    "        'experiment',\n",
    "        'result',\n",
    "        'discussion',\n",
    "        'conclusion']\n",
    "if include_appendices:\n",
    "    labels_in_order.append('appendix')\n",
    "\n",
    "print(f\"Encoding documents without max_length\")\n",
    "enc_docs = [tokenizer.encode(raw_doc) for raw_doc in raw_documents]\n",
    "\n",
    "print(\"Encoding labels...\")\n",
    "label2index = {label: idx for idx, label in enumerate(labels_in_order)}\n",
    "\n",
    "enc_labels = []\n",
    "idx = 0\n",
    "\n",
    "train_mask, valid_mask = torch.zeros(N, dtype=torch.bool), torch.zeros(N, dtype=torch.bool)\n",
    "\n",
    "for array in train_labels:\n",
    "    label_names = np.array(array)\n",
    "    array_ids = np.empty(len(unique_labels))\n",
    "    array_ids.fill(0)\n",
    "    train_mask[idx] = True\n",
    "    idx += 1\n",
    "    for label_name in label_names:\n",
    "        for label, index in label2index.items():\n",
    "            if label_name == label:\n",
    "                array_ids[index] = 1\n",
    "    enc_labels.append(array_ids)\n",
    "        \n",
    "for array in valid_labels:\n",
    "    label_names = np.array(array)\n",
    "    array_ids = np.empty(len(unique_labels))\n",
    "    array_ids.fill(0)\n",
    "    valid_mask[idx] = True\n",
    "    idx += 1\n",
    "    for label_name in label_names:\n",
    "        for label, index in label2index.items():\n",
    "            if label_name == label:\n",
    "                array_ids[index] = 1\n",
    "    enc_labels.append(array_ids)\n",
    "\n",
    "test_mask_dict = dict()\n",
    "for split in test_splits:\n",
    "    test_mask_dict[str(split)] = torch.zeros(N, dtype=torch.bool)\n",
    "    for array in test_labels_dict[str(split)]:\n",
    "        label_names = np.array(array)\n",
    "        array_ids = np.empty(len(unique_labels))\n",
    "        array_ids.fill(0)\n",
    "        test_mask_dict[str(split)][idx] = True\n",
    "        idx += 1\n",
    "        for label_name in label_names:\n",
    "            for label, index in label2index.items():\n",
    "                if label_name == label:\n",
    "                    array_ids[index] = 1\n",
    "        enc_labels.append(array_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d7213-c528-4ed4-b7e1-f315a17162cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = np.array([len(doc) for doc in enc_docs])\n",
    "print(\"Min/max document length:\", (lens.min(), lens.max()))\n",
    "print(\"Mean document length: {:.4f} ({:.4f})\".format(lens.mean(), lens.std()))\n",
    "enc_docs_arr, enc_labels_arr = np.array(enc_docs, dtype='object'), np.array(enc_labels)\n",
    "\n",
    "train_data = list(zip(enc_docs_arr[train_mask], enc_labels_arr[train_mask]))\n",
    "valid_data = list(zip(enc_docs_arr[valid_mask], enc_labels_arr[valid_mask]))\n",
    "test_data_dict = dict()\n",
    "for split in test_splits:\n",
    "    test_data_dict[str(split)] = list(zip(enc_docs_arr[test_mask_dict[str(split)]], enc_labels_arr[test_mask_dict[str(split)]]))\n",
    "\n",
    "\n",
    "print(\"N\", len(enc_docs))\n",
    "print(\"N train\", len(train_data))\n",
    "print(\"N valid\", len(valid_data))\n",
    "for split in test_splits:\n",
    "    print(\"N test ranks {}\".format(str(split)), len(test_data_dict[str(split)]))\n",
    "print(\"N classes\", len(label2index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8098eb-8ec2-4be8-b462-79ae7fbfec0d",
   "metadata": {},
   "source": [
    "# Train and evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c42a85-fb23-4188-ae4c-5e7afbbd4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tokenizers\n",
    "\n",
    "\n",
    "def collate_for_mlp(list_of_samples):\n",
    "    \"\"\" Collate function that creates batches of flat docs tensor and offsets \"\"\"\n",
    "    offset = 0\n",
    "    flat_docs, offsets, labels = [], [], []\n",
    "    for doc, label in list_of_samples:\n",
    "        if isinstance(doc, tokenizers.Encoding):\n",
    "            doc = doc.ids\n",
    "        offsets.append(offset)\n",
    "        flat_docs.extend(doc)\n",
    "        labels.append(label)\n",
    "        offset += len(doc)\n",
    "    return torch.tensor(np.array(flat_docs)), torch.tensor(np.array(offsets)), torch.tensor(np.array(labels))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_classes,\n",
    "                 num_hidden_layers=1,\n",
    "                 hidden_size=1024, hidden_act='relu',\n",
    "                 dropout=0.5, idf=None, mode='mean',\n",
    "                 pretrained_embedding=None, freeze=True,\n",
    "                 embedding_dropout=0.5):\n",
    "        nn.Module.__init__(self)\n",
    "        # Treat TF-IDF mode appropriately\n",
    "        mode = 'sum' if idf is not None else mode\n",
    "        self.idf = idf\n",
    "\n",
    "        # Input-to-hidden (efficient via embedding bag)\n",
    "        if pretrained_embedding is not None:\n",
    "            # vocabsize is defined by embedding in this case\n",
    "            self.embed = nn.EmbeddingBag.from_pretrained(pretrained_embedding, freeze=freeze, mode=mode)\n",
    "            embedding_size = pretrained_embedding.size(1)\n",
    "            self.embedding_is_pretrained = True\n",
    "        else:\n",
    "            assert vocab_size is not None\n",
    "            self.embed = nn.EmbeddingBag(vocab_size, hidden_size, mode=mode)\n",
    "            embedding_size = hidden_size\n",
    "            self.embedding_is_pretrained = False\n",
    "\n",
    "        self.activation = getattr(F, hidden_act)\n",
    "        self.embedding_dropout = nn.Dropout(embedding_dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Hidden-to-hidden\n",
    "        for i in range(num_hidden_layers - 1):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(embedding_size, hidden_size))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        # Hidden-to-output\n",
    "        self.layers.append(nn.Linear(hidden_size if self.layers else embedding_size, num_classes))\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input, offsets, labels=None):\n",
    "        # Use idf weights if present\n",
    "        idf_weights = self.idf[input] if self.idf is not None else None\n",
    "\n",
    "        h = self.embed(input, offsets, per_sample_weights=idf_weights)\n",
    "\n",
    "        if self.idf is not None:\n",
    "            # In the TF-IDF case: renormalize according to l2 norm\n",
    "            h = h / torch.linalg.norm(h, dim=1, keepdim=True)\n",
    "\n",
    "        if not self.embedding_is_pretrained:\n",
    "            # No nonlinearity when embedding is pretrained\n",
    "            h = self.activation(h)\n",
    "\n",
    "        h = self.embedding_dropout(h)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # at least one\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                # No activation/dropout for final layer\n",
    "                h = self.activation(h)\n",
    "                h = self.dropout(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(h, labels)\n",
    "            return loss, h\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c8a7d-b2bf-4f95-9542-d8bd3eaa7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LOGGING_STEPS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edcbdb-d472-44c3-a62d-c27017eef926",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing MLP\")\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "model = MLP(vocab_size, len(label2index))\n",
    "\n",
    "model.to(torch_device)\n",
    "\n",
    "if WANDB:\n",
    "    wandb.watch(model, log_freq=LOGGING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d495d-a836-48e9-b7c8-d408f97894b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss, color='red', label='loss')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.title(\"validation loss\")\n",
    "    plt.savefig(\"val_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1000a6-b34f-439a-8b83-ad6db88f25d8",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8bd08-22e5-43af-8894-b1b86cf07279",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = collate_for_mlp\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               num_workers=4,\n",
    "                                               pin_memory=('cuda' in torch_device))\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               num_workers=4,\n",
    "                                               pin_memory=('cuda' in torch_device))\n",
    "\n",
    "# len(train_loader) no of batches\n",
    "t_total = len(train_loader) // GRADIENT_ACCUMULATION_STEPS * EPOCHS\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                                num_training_steps=t_total)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Train!\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_data))\n",
    "logger.info(\"  Num Epochs = %d\", EPOCHS)\n",
    "logger.info(\"  Batch size  = %d\", BATCH_SIZE)\n",
    "logger.info(\"  Total train batch size (w. accumulation) = %d\",\n",
    "            BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "logger.info(\"  Gradient Accumulation steps = %d\", GRADIENT_ACCUMULATION_STEPS)\n",
    "logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "global_step = 0\n",
    "loss_vals = []\n",
    "tr_loss, logging_loss, nb_val_steps, vl_loss = 0.0, 0.0, 0.0, 0.0\n",
    "model.zero_grad()\n",
    "#train_iterator = trange(EPOCHS, desc=\"Epoch\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=\"Iteration\")):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(torch_device) for t in batch)\n",
    "        outputs = model(batch[0], batch[1], batch[2])\n",
    "        loss = outputs[0]\n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "            if WANDB:\n",
    "                wandb.log({'epoch': epoch,\n",
    "                            'lr': scheduler.get_last_lr()[0],\n",
    "                            'loss': loss})\n",
    "\n",
    "        if LOGGING_STEPS > 0 and global_step % LOGGING_STEPS == 0:\n",
    "            avg_loss = (tr_loss - logging_loss) / LOGGING_STEPS\n",
    "            writer.add_scalar('lr', scheduler.get_last_lr()[0], global_step)\n",
    "            writer.add_scalar('loss', avg_loss, global_step)\n",
    "            logging_loss = tr_loss\n",
    "\n",
    "    for step, batch in enumerate(tqdm(valid_loader, desc=\"Validating\")):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(torch_device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[0], batch[1], batch[2])\n",
    "\n",
    "        nb_val_steps += 1\n",
    "        loss, logits = outputs[:2]\n",
    "        vl_loss += loss.mean().item()\n",
    "\n",
    "    vl_loss /= nb_val_steps\n",
    "    loss_vals.append(vl_loss)\n",
    "    if WANDB:\n",
    "        wandb.log({'val_loss': vl_loss})\n",
    "\n",
    "writer.close()\n",
    "loss_plot(np.linspace(1, EPOCHS, EPOCHS).astype(int), loss_vals)\n",
    "#return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af14b9e-2b91-40f8-b931-61adad572a5e",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91265cf-dd24-43cd-b703-6b2eb6e1928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(MODEL_PATH+model_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e17c0-0ada-4599-a053-0377761df86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "#test specific model:\n",
    "model.load_state_dict(torch.load(MODEL_PATH+model_id))\n",
    "\n",
    "collate_fn = collate_for_mlp\n",
    "\n",
    "if validate:\n",
    "    eval_data_dict = {\"validation data\": valid_data}\n",
    "else:\n",
    "    eval_data_dict = test_data_dict\n",
    "\n",
    "for evalset in eval_data_dict:\n",
    "    print(f\"--------------- {evalset} ---------------\")\n",
    "    if (WANDB and len(eval_data_dict)>1):\n",
    "        wandb.init(project=wandb_project, name = wandb_id+\"_\"+re.sub('[\\W_]+', '', evalset), config={\"epochs\": EPOCHS, \"context_width\": context_width, \"validation\": validate, \"batch_size\": BATCH_SIZE, \"learning_rate\": LEARNING_RATE, \"lambdas\": LAMBDA, \"trainingdata\":\"full\", \"conferences\": re.sub('[\\W_]+', '', evalset)})\n",
    "    data_loader = torch.utils.data.DataLoader(eval_data_dict[evalset],\n",
    "                                                  collate_fn=collate_fn,\n",
    "                                                  num_workers=4,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  pin_memory=('cuda' in str(torch_device)),\n",
    "                                                  shuffle=False)\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    nb_eval_steps, eval_loss = 0, 0.0\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(torch_device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            # batch consist of (flat_inputs, lengths, labels)\n",
    "            outputs = model(batch[0].to(torch.long), batch[1].to(torch.long), batch[2].to(torch.float))\n",
    "            all_targets.append(batch[2].detach().cpu())\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "        # outputs [:2] should hold loss, logits\n",
    "        loss, logits = outputs[:2]\n",
    "        eval_loss += loss.mean().item()\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "\n",
    "    logits = torch.cat(all_logits)\n",
    "    logits = torch.sigmoid(logits)\n",
    "    logits = logits.numpy()\n",
    "    targets = torch.cat(all_targets).numpy()\n",
    "    eval_loss /= nb_eval_steps\n",
    "    logits[logits >= LAMBDA] = 1\n",
    "    logits[logits < LAMBDA] = 0\n",
    "    preds = logits\n",
    "    acc = accuracy_score(targets, preds)\n",
    "\n",
    "    f1_samples = f1_score(targets, preds, average='samples')\n",
    "    f1_micro = f1_score(targets, preds, average='micro')\n",
    "    f1_macro = f1_score(targets, preds, average='macro')\n",
    "\n",
    "    if WANDB:\n",
    "        wandb.log({\"test/acc\": acc, \"test/loss\": eval_loss,\n",
    "                    \"test/f1_samples\": f1_samples,\n",
    "                    \"test/f1_micro\": f1_micro,\n",
    "                    \"test/f1_macro\": f1_macro})\n",
    "\n",
    "    print(f\"Test accuracy: {acc:.4f}, f1_samples: {f1_samples:.4f}, f1_micro: {f1_micro:.4f}, f1_macro: {f1_macro:.4f}, Eval loss: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f44e2-6785-4d8d-8e6d-ac5c43fe90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(RESULTS_PATH+model_id+'_results.txt', 'w') as f:\n",
    "        print(f\"F1 Score (Samples) = {f1_samples}\",f\"Accuracy Score = {acc}\",f\"F1 Score (Micro) = {f1_micro}\",f\"F1 Score (Macro) = {f1_macro}\", file=f)\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH+model_id)\n",
    "print(\"saved as \"+MODEL_PATH+model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7eb33-5cfd-446b-8ba8-6e3581a5b467",
   "metadata": {},
   "source": [
    "#### further evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e461290-b5db-4223-b1aa-6def38f900d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report = classification_report(\n",
    "    targets,\n",
    "    preds,\n",
    "    output_dict=False,\n",
    "    target_names= labels_in_order,\n",
    "    digits = 4\n",
    ")\n",
    "with open(RESULTS_PATH+model_id+'_results.txt', 'a') as f:\n",
    "        print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1190bc-4c61-4558-bcee-7c9412a4deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_totitles(labels): #labels should be list of int or convertible to int\n",
    "    labelstring = str([int(l) for l in labels])\n",
    "    labellist = labelstring.strip('][').split(', ')\n",
    "    new_label = \"\" \n",
    "    for idx, label in enumerate(labellist):\n",
    "        if (label == \"1\"):\n",
    "            if (new_label == \"\"):\n",
    "                new_label += labels_in_order[idx]\n",
    "            else:\n",
    "                new_label = new_label + \", \" + labels_in_order[idx]\n",
    "    return new_label \n",
    "\n",
    "df_labels = pd.DataFrame()\n",
    "df_labels[\"label\"] = [label_totitles(t) for t in targets]\n",
    "df_labels[\"pred\"] = [label_totitles(p) for p in preds]\n",
    "df_labels[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion plots\n",
    "for title in set(df_labels[\"label\"]):\n",
    "    #print(\"------\"+title+\"------\")\n",
    "    df_temp = df_labels[df_labels[\"label\"] == title]\n",
    "    crosstab = pd.crosstab(df_temp[\"label\"], df_temp[\"pred\"], rownames=[\"label\"], colnames=[\"pred\"])\n",
    "    #print(crosstab)\n",
    "    ax = crosstab.plot.bar(rot=0, figsize = (25, 10), width = 0.75)\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples\n",
    "unique_preds = list(set(df_labels[\"pred\"]))\n",
    "\n",
    "if validate:\n",
    "    eval_dataset = valid_dataset\n",
    "else:\n",
    "    eval_dataset = test_dataset\n",
    "\n",
    "for category in unique_preds:\n",
    "    print(f\"----- {category} -----\")\n",
    "    df_cat = df_labels[df_labels['pred'] == category]\n",
    "    print(df_cat[\"label\"].value_counts())\n",
    "    df_cat = df_cat.sample(n = min(5, len(df_cat)))\n",
    "    for idx in df_cat.index.values:\n",
    "        print(f\"~~~ true label: {df_cat.loc[idx]['label']} ~~~\")\n",
    "        #print(fin_outputs[idx])\n",
    "        print(eval_dataset.iloc[idx][\"text\"])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
