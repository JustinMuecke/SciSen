{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa59a399-64a7-40dd-a54b-4ef1a59c7883",
   "metadata": {},
   "source": [
    "# Multi-Label Section Classifier\n",
    "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc8db8-de51-4fee-8a18-2719e5432cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import transformers\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "import re\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba36d49-b39b-40ec-9e8d-2cafbe21ae20",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58b81f-7be3-4469-9c9f-de25834c4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB = False\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b493477-3f15-47b8-a7f1-6f83736d1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the decice for GPU usage\n",
    "torch_device = 'cuda:2' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f694f9-9452-4e33-a6ef-d6edafe6e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model\n",
    "model_name = \"bert-base-uncased\" #[\"allenai/scibert_scivocab_uncased\", \"bert-base-uncased\"]\n",
    "model_id = \"bert_1s_32_1\" #for filenames\n",
    "wandb_id = \"bert_1s_32_1\"\n",
    "wandb_project = \"section-clf-multisen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b013e-a06e-4624-8e89-60b83b7cda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import parameters\n",
    "input_path = '/media/nvme3n1/proj_scisen/datasets/SciSections_sentences.jsonl'\n",
    "\n",
    "context_width = 1 #number of sentences included in the context (surrounding the target sentence, incl. target sentence) (at least 1)\n",
    "include_appendices = False #include appendices\n",
    "included_conferences = ['as','a','b','c']\n",
    "\n",
    "test_splits = [['as','a','b','c']] # [ [['as','a','b','c']], [['as'],['a'],['b','c']] ] test data split by conference rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb6811-21bc-4ee1-be3d-c9e0bb2929a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_model = '/media/nvme3n1/proj_scisen/models/MLSC/' #directory to which models are saved\n",
    "output_path_results = \"/media/nvme3n1/proj_scisen/results/MLSC/\" #directory to which further outputs are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd5dce-1598-434f-af14-cc216fbc594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate = False #evaluate based on validation set (True for hyperparameter tuning) or test set\n",
    "postprocess_labels = False\n",
    "smooth = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2845cd-7e5a-4303-9555-605e98c59cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# manual search for both models\n",
    "\n",
    "MAX_LEN = 512 #tokens\n",
    "TRAIN_BATCH_SIZE = 32 #[16,32]\n",
    "VALID_BATCH_SIZE = 32 #[16,32]\n",
    "EPOCHS = 15 # with early stopping if no improvement for past 2 epichs\n",
    "LEARNING_RATE = 1e-05 #[5e-05,3e-05,1e-05]\n",
    "\n",
    "LAMBDAS = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2] #[all 0.1, 0.2, 0.3, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f01831-0691-4aa5-9b92-96c36d3e4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    wandb.init(project=wandb_project, resume = RESUME, name = wandb_id, config={\"epochs\": EPOCHS, \"context_width\": context_width, \"validation\": validate, \"batch_size\": TRAIN_BATCH_SIZE, \"learning_rate\": LEARNING_RATE, \"lambdas\": str(LAMBDAS), \"trainingdata\":\"full\", \"conferences\": \"asabc\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbd63e-4d20-430a-b6ec-304ccad55558",
   "metadata": {},
   "source": [
    "## Import and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d787c-6061-4d59-9a03-df309c506e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load raw data\n",
    "paragraphList = list()\n",
    "with open(input_path) as f:\n",
    "    for paragraph in f:\n",
    "        paragraphDict = json.loads(paragraph)\n",
    "        paragraphList.append(paragraphDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cefd1a-a2c7-4d74-870b-9c7ab36135da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text labels for multi-label encoding and construct sentences & context\n",
    "titles = ['introduction',\n",
    "        'related work',\n",
    "        'method',\n",
    "        'experiment',\n",
    "        'result',\n",
    "        'discussion',\n",
    "        'conclusion']\n",
    "if include_appendices:\n",
    "    titles.append('appendix')\n",
    "n_sections = len(titles)\n",
    "\n",
    "def multilabel(classes):\n",
    "    classes_dict = {}\n",
    "    for title in titles:\n",
    "        classes_dict[title] = int(title in classes)\n",
    "    return classes_dict\n",
    "\n",
    "def context(sentences, sentence_idx, k_context):   \n",
    "### method of choosing sentence context ###\n",
    "#use only consecutive sentences within one paragraph (no [removed] tokens or end of paragraph):\n",
    "#k=1: no context, k=2: predecessor+sentence, k = 3: predecessor+sentence+successor, k=2n: n predecessors+S+(n-1) sucessors, k=2n+1: n predecessors+S+n sucessors\n",
    "#kick if context is empty or not of desired length or contains [removed] sentence\n",
    "    k=k_context\n",
    "    context = []\n",
    "    start = sentence_idx - k//2\n",
    "    stop = sentence_idx + k//2 if k%2 == 0 else sentence_idx + k//2 + 1\n",
    "    if start >= 0 and stop <= len(sentences):\n",
    "        context = sentences[start : stop]    \n",
    "    else:\n",
    "        return \"\"\n",
    "    if \"[removed]\" in context:\n",
    "        return \"\"\n",
    "    return \" \".join([str(item) for item in context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cd441-b856-4009-9625-2cbf1d343257",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputList = []\n",
    "for par in tqdm(paragraphList):\n",
    "    if par['section_category'] == ['appendix'] and not include_appendices:\n",
    "        continue\n",
    "    if not par['rank'] in included_conferences:\n",
    "        continue\n",
    "    label = multilabel(par['section_category'])\n",
    "    for idx, sentence in enumerate(par['sentences']):\n",
    "        if not (sentence == \"[removed]\"):\n",
    "            text_target = context(par['sentences'],idx,context_width) \n",
    "            inputList.append({**{'text_target': text_target},**label,**{'rank': par['rank']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848be9f3-31a3-4b3f-9419-b3dc24bd9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = pd.DataFrame(inputList)\n",
    "df_input=df_input.mask(df_input == '')\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c533b64-2370-4535-9ece-4d7ba88a2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect: frequencies of sections\n",
    "#sentences\n",
    "df_input['label'] = df_input[df_input.columns[1:n_sections+1]].values.tolist()\n",
    "print(df_input[~df_input['text_target'].isnull()]['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99808e9-bdd0-473e-ab96-4ff29674d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input['label'] = df_input[df_input.columns[1:n_sections+1]].values.tolist()\n",
    "new_df = df_input[['text_target','label','rank']].copy()\n",
    "new_df = new_df[~new_df['text_target'].isnull()]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd89bc1-511f-4296-a1c8-5acdf45afbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect: frequencies of ranks\n",
    "print(new_df['rank'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13230dd-8845-46df-bec6-af007b18b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller datasets for (pre-)experiments\n",
    "N = len(new_df)\n",
    "\n",
    "#random subset: 10%\n",
    "#new_df = new_df.sample(n=ceil(N*0.1), random_state = 42)\n",
    "print(new_df['rank'].value_counts())\n",
    "print(new_df['label'].value_counts())\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f6042-f4de-4c48-937c-070509bc1752",
   "metadata": {},
   "source": [
    "## Prepare the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a8e37-b7a4-4ce4-8590-f318771054df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19e183-ccc4-477e-9ed1-3d6221e12227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "# to create training and validation dataset\n",
    "# input: (BERT) tokenizer, dataframe, max_length\n",
    "# output: tokenized outputs (ids, attention_mask, token_type_ids) and tags used for BERT training\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text_target = dataframe.text_target\n",
    "        #self.text_context = dataframe.text_context\n",
    "        self.targets = dataframe.label\n",
    "        #self.targets = self.data.llist\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_target)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_target = str(self.text_target[index])\n",
    "        text_target = \" \".join(text_target.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text_target,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            #pad_to_max_length=True,\n",
    "            padding = \"max_length\",\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed56e21-8b1b-492d-9f15-7eeb35f44f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "\n",
    "# split: 70/20/10\n",
    "train_size = 0.7\n",
    "valid_size = 0.2\n",
    "\n",
    "#split into train, test:\n",
    "trainvalid_df=new_df.sample(frac= train_size + valid_size ,random_state=200)\n",
    "test_dataset=new_df.drop(trainvalid_df.index).reset_index(drop=True)\n",
    "trainvalid_df = trainvalid_df.reset_index(drop=True)\n",
    "\n",
    "#split into train, validation:\n",
    "train_dataset=trainvalid_df.sample(frac= train_size/(train_size + valid_size) ,random_state=200)\n",
    "valid_dataset=trainvalid_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "if smooth:\n",
    "    train_dataset[\"label\"] = [[(lambda x: x-0.1 if x == 1 else x+0.1)(x) for x in labels] for labels in train_dataset[\"label\"]]\n",
    "\n",
    "#split test by conferences:\n",
    "test_datasets = dict()\n",
    "for split in test_splits:\n",
    "    test_datasets[str(split)]=test_dataset[test_dataset['rank'].isin(split)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(valid_dataset.shape))\n",
    "print(\"FULL TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "for split in test_splits:\n",
    "    print(\"TEST Dataset ranks {}: {}\".format(str(split), test_datasets[str(split)].shape))\n",
    "\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN) #70% of original data, used to fine tune the model\n",
    "validation_set = CustomDataset(valid_dataset, tokenizer, MAX_LEN) #20% of original data, used to evaluate the performance of the model during training\n",
    "\n",
    "testing_sets = dict() #10% of original data, used to evaluate the performance of the trained model\n",
    "for split in test_splits:\n",
    "    testing_sets[str(split)] = CustomDataset(test_datasets[str(split)], tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "# Creating the dataloader\n",
    "#used for creating training and validation dataloader that load data to the neural network in a defined manner.\n",
    "#This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "#This control is achieved using the parameters such as batch_size and max_len\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **test_params)\n",
    "#testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "testing_loaders = dict()\n",
    "for split in test_splits:\n",
    "    testing_loaders[str(split)] = DataLoader(testing_sets[str(split)], **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbb94f-cda3-46e5-a5b5-20e0c1c951e9",
   "metadata": {},
   "source": [
    "## Create the Network for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46abb6-8d66-410e-9925-9e3507487bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "# BERT model + Dropout (to regularise) + Linear (to classify) Layer\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained(model_name, return_dict = False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 7) # as many dimensions as categories #[7,2]\n",
    "\n",
    "    def forward(self, ids, mask,token_type_ids):\n",
    "        hidden_state, pooled_output = self.l1(ids, attention_mask=mask,token_type_ids=token_type_ids) #BERTModel layer\n",
    "        output_2 = self.l2(pooled_output) #dropout layer\n",
    "        output = self.l3(output_2) #linear layer\n",
    "        return output # final layer output used to calculate loss and determine accuracy\n",
    "\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(torch_device)\n",
    "if WANDB:\n",
    "    wandb.watch(model, log_freq = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c9714-ba98-4c28-a4ab-bc7441be2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "# use final layer output to calculate loss and determine accuracy\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets) # binary cross-entropy with logits loss for multilabel classification\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecabac-bd12-4512-8dd7-37baf725d2a0",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ff72c-4ff1-4988-b808-8488fe898953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Val loss\n",
    "def loss_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss, color='red', label='loss')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.title(\"validation loss\")\n",
    "    plt.savefig(output_path_results+model_id+\"_val_loss.png\")\n",
    "    \n",
    "loss_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753687e8-569e-4b16-9624-3226b391e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "n_epochs_stop = 2\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "min_val_loss = np.Inf\n",
    "best_model_name = \"\"\n",
    "final_epoch = 0\n",
    "best_epoch = 0\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    final_epoch += 1\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx,data in enumerate(tqdm(training_loader), 0): #dataloader passes data to the model based on batch size\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward\n",
    "        ids = data['ids'].to(torch_device, dtype = torch.long)\n",
    "        mask = data['mask'].to(torch_device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(torch_device, dtype = torch.long)\n",
    "        targets = data['targets'].to(torch_device, dtype = torch.float)\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        \n",
    "        #backward\n",
    "        loss = loss_fn(outputs, targets) #output from the model and the actual category are compared to calculate the loss   \n",
    "        loss.backward() #loss value is used to optimize the weights of the neurons in the network\n",
    "        optimizer.step()\n",
    "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
    "        if WANDB:\n",
    "                wandb.log({'epoch': epoch,\n",
    "                            'loss': loss})\n",
    "        \n",
    "    #validate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['ids'].to(torch_device, dtype = torch.long)\n",
    "            mask = data['mask'].to(torch_device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(torch_device, dtype = torch.long)\n",
    "            targets = data['targets'].to(torch_device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets) # validation loss\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss / len(training_loader)\n",
    "        valid_loss = valid_loss / len(validation_loader)\n",
    "        \n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
    "            epoch,\n",
    "             train_loss,\n",
    "            valid_loss\n",
    "        ))\n",
    "        loss_vals.append(valid_loss)\n",
    "        if WANDB:\n",
    "                wandb.log({'val_loss': valid_loss})\n",
    "        \n",
    "    #early stopping\n",
    "    # reference: https://www.kaggle.com/code/akhileshrai/tutorial-early-stopping-vanilla-rnn-pytorch/notebook\n",
    "    # If the validation loss is at a minimum\n",
    "    if valid_loss < min_val_loss:\n",
    "        #Save the model\n",
    "        best_model_name = output_path_model+model_id+'_best_epoch'+str(epoch)\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), best_model_name)\n",
    "        epochs_no_improve = 0\n",
    "        min_val_loss = valid_loss\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    if epochs_no_improve == n_epochs_stop: #and epoch > 2 \n",
    "        print('Early stopping!' )\n",
    "        early_stop = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106dfd1-632f-4cfc-884e-cd3d6a039d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    wandb.config.update({\"best_epochs\": best_epoch+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10572a40-ccb7-4227-83f4-fbff87870ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "loss_plot(np.linspace(1, final_epoch, final_epoch).astype(int), loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f4075-7952-4312-aa91-6b2a85fe4b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_name))\n",
    "torch.save(model.state_dict(), output_path_model+model_id)\n",
    "print(\"saved as \"+output_path_model+model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1c399-c821-46b3-9fb4-7eb279b4ef8d",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f10558-ace1-4fc6-8f36-a0c58bf365c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model\n",
    "\n",
    "#model.load_state_dict(torch.load(best_model_name))\n",
    "model.load_state_dict(torch.load(output_path_model+model_id))\n",
    "\n",
    "if validate:\n",
    "    eval_loaders = {\"validation set\": validation_loader}\n",
    "else:\n",
    "    eval_loaders = testing_loaders\n",
    "    outputs_by_rank = {}\n",
    "    targets_by_rank = {}\n",
    "\n",
    "for evalset in eval_loaders:\n",
    "    print(f\"--------------- {evalset} ---------------\")\n",
    "    if (WANDB and len(eval_loaders)>1):\n",
    "        wandb.init(project=wandb_project, resume = RESUME, name = wandb_id+\"_\"+re.sub('[\\W_]+', '', evalset), config={\"epochs\": EPOCHS, \"context_width\": context_width, \"validation\": validate, \"batch_size\": TRAIN_BATCH_SIZE, \"learning_rate\": LEARNING_RATE, \"lambdas\": str(LAMBDAS), \"trainingdata\":\"full\", \"conferences\": re.sub('[\\W_]+', '', evalset)})\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(tqdm(eval_loaders[evalset], 0)):\n",
    "            ids = data['ids'].to(torch_device, dtype=torch.long)\n",
    "            mask = data['mask'].to(torch_device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(torch_device, dtype = torch.long)\n",
    "            targets = data['targets'].to(torch_device, dtype=torch.float)\n",
    "            outputs = model(ids, mask,token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "    outputs = fin_outputs\n",
    "    targets = fin_targets    \n",
    "\n",
    "    #outputs = np.array(outputs) >= LAMBDA\n",
    "    outputs = np.array(outputs) >= LAMBDAS\n",
    "    \n",
    "    if postprocess_labels:\n",
    "        for idx, output in enumerate(outputs):\n",
    "            n_labels = sum(output)\n",
    "            if not n_labels in [1,2]:\n",
    "                fin_output = fin_outputs[idx][:]\n",
    "                new_output = [False] * len(titles)\n",
    "                if n_labels == 0:\n",
    "                    new_output[fin_output.index(max(fin_output))] = True\n",
    "                else: # n_labels > 2:\n",
    "                    while (sum(new_output) < 2):\n",
    "                        new_output[fin_output.index(max(fin_output))] = True\n",
    "                        fin_output[fin_output.index(max(fin_output))] = 0\n",
    "                outputs[idx] = new_output\n",
    "    \n",
    "    \n",
    "    if not validate:\n",
    "        outputs_by_rank[evalset] = outputs\n",
    "        targets_by_rank[evalset] = targets\n",
    "          \n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_avg = metrics.f1_score(targets, outputs, average='samples')\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "\n",
    "\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Samples) = {f1_score_avg}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "          \n",
    "       \n",
    "    if WANDB:\n",
    "        wandb.config.update({\"test data\": evalset})\n",
    "        wandb.log({\"test/acc\": accuracy,\n",
    "                    \"test/f1_samples\": f1_score_avg,\n",
    "                    \"test/f1_micro\": f1_score_micro,\n",
    "                    \"test/f1_macro\": f1_score_macro})\n",
    "      \n",
    "    classification_report = metrics.classification_report(\n",
    "    targets,\n",
    "    outputs,\n",
    "    output_dict=False,\n",
    "    target_names= titles,\n",
    "    digits = 4)\n",
    "\n",
    "    with open(output_path_results+model_id+evalset+\"_results.txt\", \"w\") as f:\n",
    "        print(f\"F1 Score (Samples) = {f1_score_avg}\",f\"Accuracy Score = {accuracy}\",f\"F1 Score (Micro) = {f1_score_micro}\",f\"F1 Score (Macro) = {f1_score_macro}\", file=f)\n",
    "        print(\"--- Classification Report: ---\")\n",
    "        print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a6b1c-87da-4252-b9d3-3759d0eeeabf",
   "metadata": {},
   "source": [
    "### further evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584a726-7244-4bb3-81d3-f556e51f17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_totitles(labels):\n",
    "    #display section names\n",
    "    #labels should be list of int or convertible to int\n",
    "    labelstring = str([int(l) for l in labels])\n",
    "    labellist = labelstring.strip('][').split(', ')\n",
    "    new_label = \"\" \n",
    "    for idx, label in enumerate(labellist):\n",
    "        if (label == \"1\"):\n",
    "            if (new_label == \"\"):\n",
    "                new_label += titles[idx]\n",
    "            else:\n",
    "                new_label = new_label + \", \" + titles[idx]\n",
    "    return new_label \n",
    "\n",
    "df_labels = pd.DataFrame()\n",
    "df_labels[\"label\"] = [label_totitles(t) for t in targets]\n",
    "df_labels[\"pred\"] = [label_totitles(o) for o in outputs]\n",
    "print(df_labels[\"pred\"].value_counts())\n",
    "#? df_labels[\"ids\"] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e60e0-74d6-468a-be9a-ff6f3a8d1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_labels[\"label\"]))\n",
    "#print(len(df_labels[len(df_labels['label']) > 2]))\n",
    "print(len(outputs))\n",
    "niceos = [o for o in outputs if sum(o)<=2]\n",
    "bados = [o for o in outputs if sum(o)>2]\n",
    "print(len(niceos))\n",
    "print(len(bados))\n",
    "print(len(niceos)+len(bados))\n",
    "\n",
    "len(bados)/len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb8aae-3e25-428a-93ae-bfa07fc265a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scisensection(model, text_target, tokenizer, MAX_LEN):\n",
    "    #get outputs for single sentence\n",
    "    model.eval() \n",
    "    inputs = tokenizer.encode_plus(\n",
    "            text_target,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True,\n",
    "            #pad_to_max_length=True,\n",
    "            padding = \"max_length\",\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long).unsqueeze(0).to(torch_device, dtype=torch.long)\n",
    "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).unsqueeze(0).to(torch_device, dtype=torch.long)\n",
    "    token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).to(torch_device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        output = model(ids, mask, token_type_ids)\n",
    "        fin_output = torch.sigmoid(output)\n",
    "        fin_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
    "    output= np.array(fin_output) >= LAMBDAS\n",
    "    return label_totitles(output[0]), fin_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66495b-931d-48a2-9676-45402c5114e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_target = \"Prior research has shown that this works.\"\n",
    "scisensection(model, text_target, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb06cc1-4725-4d4e-84ae-d310ac0bcd35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### examples and confusion plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcbb95-de88-40fa-b1a5-06067a48790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples per category\n",
    "# for each predicted label combination, print:\n",
    "# number of true labels that have been classified here\n",
    "# up to 5 example sentences incl. their true labels\n",
    "\n",
    "unique_preds = list(set(df_labels[\"pred\"]))\n",
    "\n",
    "if validate:\n",
    "    eval_dataset = valid_dataset\n",
    "else:\n",
    "    eval_dataset = test_dataset\n",
    "\n",
    "for category in unique_preds:\n",
    "    print(f\"----- {category} -----\")\n",
    "    df_cat = df_labels[df_labels['pred'] == category]\n",
    "    print(df_cat[\"label\"].value_counts())\n",
    "    df_cat = df_cat.sample(n = min(5, len(df_cat)))\n",
    "    for idx in df_cat.index.values:\n",
    "        print(f\"~~~ true label: {df_cat.loc[idx]['label']} ~~~\")\n",
    "        #print(fin_outputs[idx])\n",
    "        print(eval_dataset.iloc[idx][\"text_target\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion plots\n",
    "\n",
    "for title in set(df_labels[\"label\"]):\n",
    "    df_temp = df_labels[df_labels[\"label\"] == title]\n",
    "    crosstab = pd.crosstab(df_temp[\"label\"], df_temp[\"pred\"], rownames=[\"label\"], colnames=[\"pred\"])\n",
    "    ax = crosstab.plot.bar(rot=0, figsize = (25, 10), width = 0.75)\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
